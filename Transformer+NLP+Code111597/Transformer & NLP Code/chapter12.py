# ä¾‹12-1
# å¼•å…¥æ‰€éœ€åº“
import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset

# å‡è®¾æ•°æ®é›†åŒ…å«ä¸¤åˆ—ï¼š 'text'ï¼ˆç”¨æˆ·è¯„è®ºæ–‡æœ¬ï¼‰å’Œ 'label'ï¼ˆæƒ…æ„Ÿæ ‡ç­¾ï¼Œå¦‚ 0 è¡¨ç¤ºè´Ÿå‘ï¼Œ1 è¡¨ç¤ºä¸­æ€§ï¼Œ2 è¡¨ç¤ºæ­£å‘ï¼‰
data = pd.DataFrame({
    "text": ["æˆ‘å¾ˆå–œæ¬¢è¿™ä¸ªäº§å“ï¼", "æœåŠ¡æ€åº¦å·®", "è´¨é‡ä¸é”™ï¼Œä½†æ˜¯æœ‰ç‚¹è´µ", "éå¸¸æ»¡æ„ï¼Œä¸‹æ¬¡è¿˜ä¼šè´­ä¹°"],
    "label": [2, 0, 1, 2]
})

# æ•°æ®é¢„å¤„ç†
def preprocess_data(data, max_length=128):
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")  # ä½¿ç”¨ä¸­æ–‡BERT
    # å¯¹æ•°æ®è¿›è¡Œç¼–ç 
    encodings = tokenizer(data['text'].tolist(), truncation=True, padding=True, max_length=max_length)
    return encodings

# åŠ è½½æ•°æ®å¹¶åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
train_encodings = preprocess_data(train_data)
test_encodings = preprocess_data(test_data)

# è½¬æ¢ä¸ºDatasetæ ¼å¼
train_dataset = Dataset.from_dict({"input_ids": train_encodings["input_ids"], "attention_mask": train_encodings["attention_mask"], "labels": train_data['label'].tolist()})
test_dataset = Dataset.from_dict({"input_ids": test_encodings["input_ids"], "attention_mask": test_encodings["attention_mask"], "labels": test_data['label'].tolist()})
æ­¤å¤„çš„ä»£ç å®Œæˆäº†æ•°æ®çš„åŠ è½½ä¸ç¼–ç ï¼ŒBertTokenizerç”¨æ¥å°†æ–‡æœ¬è½¬åŒ–ä¸ºæ¨¡å‹å¯æ¥å—çš„è¾“å…¥æ ¼å¼ï¼Œæ•°æ®é›†ä¸­åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä»¥ä¾¿åç»­æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚BERTçš„æœ€å¤§è¾“å…¥é•¿åº¦è®¾ç½®ä¸º128ä»¥é€‚åº”ä¸åŒçš„æ–‡æœ¬ã€‚
æ¥ä¸‹æ¥ï¼Œé€šè¿‡BertForSequenceClassificationåˆå§‹åŒ–BERTæ¨¡å‹ï¼Œç”¨äºæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼Œé€‰æ‹©åˆé€‚çš„è®­ç»ƒå‚æ•°ï¼Œå¹¶å¯åŠ¨æ¨¡å‹è®­ç»ƒã€‚
# æ¨¡å‹åˆå§‹åŒ–
model = BertForSequenceClassification.from_pretrained("bert-base-chinese", num_labels=3)

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",          # è¾“å‡ºç›®å½•
    evaluation_strategy="epoch",     # æ¯ä¸ªepochåè¿›è¡Œè¯„ä¼°
    per_device_train_batch_size=8,   # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=8,    # æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼°æ‰¹æ¬¡å¤§å°
    num_train_epochs=3,              # è®­ç»ƒçš„æ€»epochæ•°
    logging_dir='./logs',            # æ—¥å¿—ä¿å­˜ç›®å½•
)

# ä½¿ç”¨ Trainer API è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°
trainer = Trainer(
    model=model,                         # æ¨¡å‹
    args=training_args,                  # è®­ç»ƒå‚æ•°
    train_dataset=train_dataset,         # è®­ç»ƒæ•°æ®é›†
    eval_dataset=test_dataset            # æµ‹è¯•æ•°æ®é›†
)

# æ¨¡å‹è®­ç»ƒ
trainer.train()
æ­¤éƒ¨åˆ†ä»£ç é€šè¿‡Traineræ¥å£å®šä¹‰äº†è®­ç»ƒå‚æ•°ï¼Œå¹¶è®¾å®šäº†è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€è®­ç»ƒè½®æ•°ã€è¯„ä¼°ç­–ç•¥ç­‰ç»†èŠ‚ã€‚è®­ç»ƒæ¨¡å‹åï¼ŒBERTæ¨¡å‹å°†ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€çŸ¥è¯†æ¥åˆ†æå¹¶åˆ†ç±»æƒ…æ„Ÿã€‚
è®­ç»ƒå®Œæˆåï¼Œæ¥ä¸‹æ¥å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œä»¥ç¡®è®¤æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚
# æ¨¡å‹è¯„ä¼°
eval_results = trainer.evaluate()
print(f"æ¨¡å‹è¯„ä¼°ç»“æœ: {eval_results}")


# ä¾‹12-2
import random
import jieba
from synonyms import synonyms
from opencc import OpenCC

# åŠ è½½åŸå§‹æ•°æ®
data = [
    {"text": "è¿™æ¬¾äº§å“éå¸¸å¥½ç”¨ï¼ŒåŠŸèƒ½å¼ºå¤§ä¸”æ˜“æ“ä½œ", "label": "positive"},
    {"text": "æœåŠ¡æ€åº¦å·®ï¼Œä½“éªŒéå¸¸ç³Ÿç³•", "label": "negative"},
    {"text": "äº§å“è´¨é‡ä¸€èˆ¬ï¼Œä½†ä»·æ ¼å®æƒ ", "label": "neutral"}
]

# è¿‘ä¹‰è¯æ›¿æ¢å‡½æ•°
def synonym_replacement(text, replace_prob=0.3):
    words = jieba.lcut(text)
    new_words = []
    for word in words:
        if random.random() < replace_prob:
            similar_words = synonyms.nearby(word)
            if similar_words:  # å¦‚æœæœ‰è¿‘ä¹‰è¯
                word = random.choice(similar_words[0])
        new_words.append(word)
    return ''.join(new_words)

# ç®€ç¹ä½“è½¬æ¢å‡½æ•°
cc = OpenCC('s2t')
def convert_simplified_to_traditional(text):
    return cc.convert(text)

# æ‹¼å†™å˜åŒ–å‡½æ•°ï¼ˆé€‚ç”¨äºä¸­æ–‡æ‹¼éŸ³ç›¸ä¼¼çš„æ›¿æ¢ï¼‰
def typo_augmentation(text, typo_prob=0.2):
    typo_dict = {'å¥½': 'å·', 'å·®': 'æŸ¥', 'å¼º': 'å¢™', 'æ˜“': 'ä¾'}
    words = list(text)
    for i, word in enumerate(words):
        if random.random() < typo_prob and word in typo_dict:
            words[i] = typo_dict[word]
    return ''.join(words)

# æ‰©å……æ•°æ®
def augment_data(data):
    augmented_data = []
    for entry in data:
        text = entry['text']
        label = entry['label']

        # åŸæ–‡æ•°æ®
        augmented_data.append({"text": text, "label": label})
        
        # è¿‘ä¹‰è¯æ›¿æ¢
        augmented_text = synonym_replacement(text)
        augmented_data.append({"text": augmented_text, "label": label})
        
        # ç®€ç¹ä½“è½¬æ¢
        traditional_text = convert_simplified_to_traditional(text)
        augmented_data.append({"text": traditional_text, "label": label})
        
        # æ‹¼å†™å˜åŒ–
        typo_text = typo_augmentation(text)
        augmented_data.append({"text": typo_text, "label": label})

    return augmented_data

# è¿è¡Œå¢å¼ºä»£ç å¹¶å±•ç¤ºç»“æœ
augmented_data = augment_data(data)
for entry in augmented_data:
    print(f"Text: {entry['text']}, Label: {entry['label']}")


# ä¾‹12-3
# å¯¼å…¥æ‰€éœ€çš„åº“
from sentence_transformers import SentenceTransformer
import numpy as np

# åˆå§‹åŒ–SBERTæ¨¡å‹
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# æ ·æœ¬æ•°æ®ï¼ŒåŒ…å«ä¸åŒæƒ…æ„Ÿç±»åˆ«çš„å¥å­
texts = [
    "è¿™æ¬¾äº§å“éå¸¸å¥½ç”¨ï¼ŒåŠŸèƒ½å¼ºå¤§ä¸”æ˜“æ“ä½œã€‚",
    "æœåŠ¡æ€åº¦å·®ï¼Œä½“éªŒéå¸¸ç³Ÿç³•ã€‚",
    "äº§å“è´¨é‡ä¸€èˆ¬ï¼Œä½†ä»·æ ¼å®æƒ ã€‚",
    "è¿™æ˜¯æˆ‘ç”¨è¿‡çš„æœ€å¥½çš„ä¸€æ¬¾åº”ç”¨ã€‚",
    "è¿™å®¶é¤å…çš„æœåŠ¡çœŸçš„å¾ˆå·®åŠ²ã€‚",
    "è¿™ä»¶å•†å“çš„æ€§ä»·æ¯”éå¸¸é«˜ï¼Œå€¼å¾—æ¨èï¼"
]

# ä½¿ç”¨SBERTç”Ÿæˆæ–‡æœ¬åµŒå…¥
embeddings = model.encode(texts)

# è¾“å‡ºæ¯æ¡æ–‡æœ¬çš„åµŒå…¥å‘é‡
for i, embedding in enumerate(embeddings):
    print(f"Text: {texts[i]}")
    print(f"Embedding: {embedding}\n")


# ä¾‹12-4
import numpy as np

# ç¤ºä¾‹æƒ…æ„Ÿè¯å…¸
sentiment_dict = {"å¥½ç”¨": 1, "å¼ºå¤§": 1, "å·®åŠ²": -1, "æ¨è": 1, "ç³Ÿç³•": -1}

def enhance_embedding(text, embedding):
    words = text.split(" ")
    weights = [sentiment_dict.get(word, 0) for word in words]
    # è®¡ç®—æƒ…æ„Ÿå¢å¼ºåçš„æƒé‡
    enhancement_factor = np.mean(weights)
    return embedding * (1 + enhancement_factor)

# åº”ç”¨æƒ…æ„Ÿå¢å¼ºåˆ°æ¯ä¸ªæ–‡æœ¬çš„åµŒå…¥
enhanced_embeddings = [enhance_embedding(text, emb) for text, emb in zip(texts, embeddings)]
for i, enhanced_embedding in enumerate(enhanced_embeddings):
    print(f"Text: {texts[i]}")
    print(f"Enhanced Embedding: {enhanced_embedding}\n")
åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæƒ…æ„Ÿè¯å…¸å¯¹æ¯ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡è¿›è¡ŒåŠ æƒå¢å¼ºï¼Œä½¿æ¨¡å‹æ›´å…³æ³¨æƒ…æ„Ÿè¯æ±‡çš„æƒé‡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æƒ…æ„Ÿå‘é‡ä¿¡æ¯ï¼Œä»è€Œæå‡æƒ…æ„Ÿåˆ†ç±»å’Œèšç±»æ•ˆæœã€‚
ï¼ˆ3ï¼‰åº”ç”¨é™ç»´æ–¹æ³•ä¼˜åŒ–åµŒå…¥
å½“ç”Ÿæˆçš„åµŒå…¥ç»´åº¦è¾ƒé«˜æ—¶ï¼Œå¯ä»¥é€šè¿‡é™ç»´ç®—æ³•é™ä½å…¶å¤æ‚æ€§ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å¼€é”€å’Œå­˜å‚¨éœ€æ±‚ã€‚å¸¸ç”¨çš„é™ç»´æ–¹æ³•åŒ…æ‹¬PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰å’Œt-SNEï¼Œåœ¨èšç±»å’Œç›¸ä¼¼åº¦ä»»åŠ¡ä¸­å¯ä»¥å¸¦æ¥æ›´é«˜çš„æ•ˆç‡ã€‚
from sklearn.decomposition import PCA

# ä½¿ç”¨PCAå°†åµŒå…¥å‘é‡é™åˆ°50ç»´
pca = PCA(n_components=50)
reduced_embeddings = pca.fit_transform(enhanced_embeddings)

# è¾“å‡ºé™ç»´ç»“æœ
for i, reduced_embedding in enumerate(reduced_embeddings):
    print(f"Text: {texts[i]}")
    print(f"Reduced Embedding (50D): {reduced_embedding}\n")


# ä¾‹12-5
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer
import pandas as pd
åœ¨12.1.2ä¸­æˆ‘ä»¬å·²ç»å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œäº†æƒ…æ„Ÿåˆ†ç±»ï¼Œå¹¶ç”Ÿæˆäº†æ¯æ¡æ–‡æœ¬çš„åµŒå…¥å‘é‡ã€‚ä¸ºä¾¿äºæ¼”ç¤ºï¼Œä¸‹é¢åˆ›å»ºä¸€äº›ç¤ºä¾‹æ•°æ®ï¼Œå…¶ä¸­åŒ…å«æ¯æ¡æ–‡æœ¬çš„æƒ…æ„Ÿç±»åˆ«å’ŒåµŒå…¥å‘é‡ï¼š
# ç¤ºä¾‹æ–‡æœ¬æ•°æ®å’Œæƒ…æ„Ÿåˆ†ç±»ç»“æœ
texts = [
    "äº§å“éå¸¸å¥½ç”¨ï¼Œå¼ºçƒˆæ¨è", 
    "éå¸¸ä¸æ»¡æ„ï¼Œè´¨é‡å·®", 
    "æœåŠ¡ä¸é”™ï¼Œå€¼å¾—æ¨è",
    "æ€§ä»·æ¯”å¾ˆé«˜ï¼Œéå¸¸åˆ’ç®—",
    "å¾ˆå¤±æœ›ï¼Œä¸ä¼šå†è´­ä¹°"
]

# ä½¿ç”¨Sentence-BERTæ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = model.encode(texts)

# æŸ¥çœ‹ç”Ÿæˆçš„åµŒå…¥å‘é‡
for i, embedding in enumerate(embeddings):
    print(f"Text: {texts[i]}")
    print(f"Embedding: {embedding[:5]}... (length: {len(embedding)})\n")
ä½¿ç”¨K-meansèšç±»ç®—æ³•å¯¹åµŒå…¥å‘é‡è¿›è¡Œèšç±»ã€‚å‡è®¾æƒ…æ„Ÿç±»åˆ«åˆ†ä¸ºä¸¤ç±»ï¼ˆæ­£é¢ã€è´Ÿé¢ï¼‰ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥æ ¹æ®æ•°æ®è§„æ¨¡å’Œæƒ…æ„Ÿç±»å‹é€‰æ‹©ä¸åŒçš„èšç±»æ•°ï¼š
# å®šä¹‰èšç±»æ•°ï¼ˆä¾‹å¦‚ï¼Œåˆ†ä¸ºä¸¤ç±»ï¼šæ­£é¢ã€è´Ÿé¢ï¼‰
num_clusters = 2

# åˆå§‹åŒ–KMeansæ¨¡å‹å¹¶è¿›è¡Œèšç±»
kmeans = KMeans(n_clusters=num_clusters, random_state=0)
kmeans.fit(embeddings)

# è·å–æ¯ä¸ªæ–‡æœ¬çš„èšç±»æ ‡ç­¾
cluster_labels = kmeans.labels_

# æ‰“å°æ¯æ¡æ–‡æœ¬çš„èšç±»ç»“æœ
for i, label in enumerate(cluster_labels):
    print(f"Text: {texts[i]} -> Cluster Label: {label}")



# ä¾‹12-6
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers.onnx import export
from onnxruntime import InferenceSession
import numpy as np

# Step 1: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# è®¾ç½®è®¾å¤‡ä¸ºCPU
device = torch.device("cpu")
model.to(device)
model.eval()

# Step 2: å®šä¹‰ONNXå¯¼å‡ºçš„è·¯å¾„
onnx_path = "bert_model.onnx"

# Step 3: åˆ›å»ºä¸€ä¸ªç”¨äºONNXå¯¼å‡ºçš„ç¤ºä¾‹è¾“å…¥
dummy_input = tokenizer("This is a sample input for ONNX conversion.", 
                        return_tensors="pt", 
                        padding="max_length", 
                        max_length=128, 
                        truncation=True)

# å°†ç¤ºä¾‹è¾“å…¥è½¬ä¸ºPyTorchå¼ é‡
input_ids = dummy_input["input_ids"].to(device)
attention_mask = dummy_input["attention_mask"].to(device)

# Step 4: å¯¼å‡ºæ¨¡å‹ä¸ºONNX
export(model=model,
       tokenizer=tokenizer,
       opset=11,  # ONNXçš„opsetç‰ˆæœ¬
       output=onnx_path,
       input_names=["input_ids", "attention_mask"],
       dynamic_axes={
           "input_ids": {0: "batch_size", 1: "sequence_length"},
           "attention_mask": {0: "batch_size", 1: "sequence_length"}
       })

print(f"ONNXæ¨¡å‹å·²å¯¼å‡ºè‡³ {onnx_path}")

# Step 5: ä½¿ç”¨ONNX RuntimeåŠ è½½æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†éªŒè¯
session = InferenceSession(onnx_path)

# å‡†å¤‡è¾“å…¥æ•°æ®
onnx_inputs = {
    "input_ids": input_ids.cpu().numpy(),
    "attention_mask": attention_mask.cpu().numpy()
}

# ä½¿ç”¨ONNXæ¨¡å‹è¿›è¡Œæ¨ç†
outputs = session.run(None, onnx_inputs)

# éªŒè¯è¾“å‡º
logits = outputs[0]
predicted_class = np.argmax(logits, axis=1)

print("ONNXæ¨ç†ç»“æœ:", logits)
print("é¢„æµ‹ç±»åˆ«:", predicted_class)

# Step 6: æ¯”è¾ƒPyTorchä¸ONNXçš„æ¨ç†ç»“æœ
with torch.no_grad():
    torch_outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    torch_logits = torch_outputs.logits.cpu().numpy()

print("PyTorchæ¨ç†ç»“æœ:", torch_logits)
np.testing.assert_allclose(torch_logits, logits, rtol=1e-3, atol=1e-5)
print("PyTorchä¸ONNXæ¨ç†ç»“æœä¸€è‡´")



# ä¾‹12-7
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# åŠ è½½åˆ†è¯å™¨å’Œé¢„è®­ç»ƒæ¨¡å‹
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
model.eval()

# ç¤ºä¾‹è¾“å…¥æ–‡æœ¬
texts = ["I love this product!", "This is a bad experience."]
å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥æ¥å—çš„è¾“å…¥æ ¼å¼ï¼Œç¡®ä¿ä¸ONNXè½¬æ¢æ—¶çš„è¾“å…¥ç»“æ„ä¸€è‡´ï¼š
# åˆ†è¯å¤„ç†
encoded_inputs = tokenizer(
    texts,
    padding="max_length",
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

input_ids = encoded_inputs["input_ids"]
attention_mask = encoded_inputs["attention_mask"]
ä½¿ç”¨Hugging Faceæä¾›çš„ONNXå¯¼å‡ºå·¥å…·ï¼Œå°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ¨¡å‹ï¼š
from transformers.onnx import export

# å®šä¹‰ONNXå¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
onnx_path = "bert_sentiment.onnx"

# å¯¼å‡ºONNXæ¨¡å‹
export(
    model=model,
    tokenizer=tokenizer,
    output=onnx_path,
    opset=11,  # ONNXæ“ä½œé›†ç‰ˆæœ¬
    input_names=["input_ids", "attention_mask"],
    dynamic_axes={
        "input_ids": {0: "batch_size", 1: "sequence_length"},
        "attention_mask": {0: "batch_size", 1: "sequence_length"}
    }
)

print(f"ONNXæ¨¡å‹å·²æˆåŠŸä¿å­˜è‡³: {onnx_path}")
é€šè¿‡ONNX RuntimeåŠ è½½æ¨¡å‹ï¼Œå¹¶å¯¹ç¤ºä¾‹æ–‡æœ¬è¿›è¡Œæ¨ç†ï¼ŒéªŒè¯ONNXæ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š
import numpy as np
from onnxruntime import InferenceSession

# åŠ è½½ONNXæ¨¡å‹
session = InferenceSession(onnx_path)

# å‡†å¤‡è¾“å…¥æ•°æ®
onnx_inputs = {
    "input_ids": input_ids.numpy(),
    "attention_mask": attention_mask.numpy()
}

# æ¨ç†
onnx_outputs = session.run(None, onnx_inputs)
onnx_logits = onnx_outputs[0]

# è®¡ç®—ç±»åˆ«
predicted_classes = np.argmax(onnx_logits, axis=1)
print("ONNXæ¨¡å‹é¢„æµ‹ç»“æœ:", predicted_classes)
é€šè¿‡æ¯”è¾ƒPyTorchå’ŒONNXæ¨ç†ç»“æœï¼ŒéªŒè¯å¯¼å‡ºçš„æ¨¡å‹æ˜¯å¦æ­£ç¡®ï¼š
with torch.no_grad():
    torch_outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    torch_logits = torch_outputs.logits.numpy()

print("PyTorchæ¨¡å‹é¢„æµ‹ç»“æœ:", np.argmax(torch_logits, axis=1))

# éªŒè¯ç»“æœæ˜¯å¦ä¸€è‡´
np.testing.assert_allclose(torch_logits, onnx_logits, rtol=1e-3, atol=1e-5)
print("PyTorchä¸ONNXæ¨ç†ç»“æœä¸€è‡´")
è¿è¡Œç»“æœå¦‚ä¸‹ï¼š
ONNXæ¨¡å‹å·²æˆåŠŸä¿å­˜è‡³: bert_sentiment.onnx
ONNXæ¨¡å‹é¢„æµ‹ç»“æœ: [0 1]
PyTorchæ¨¡å‹é¢„æµ‹ç»“æœ: [0 1]
PyTorchä¸ONNXæ¨ç†ç»“æœä¸€è‡´


# ä¾‹12-8
import tensorrt as trt
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit

# TensorRTæ—¥å¿—è®°å½•å™¨
logger = trt.Logger(trt.Logger.WARNING)

# å®šä¹‰ONNXæ¨¡å‹è·¯å¾„å’ŒTensorRTå¼•æ“è·¯å¾„
onnx_model_path = "bert_sentiment.onnx"
trt_engine_path = "bert_sentiment.trt"

# åˆ›å»ºTensorRTæ„å»ºå™¨å’Œç½‘ç»œå®šä¹‰
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, logger)

# åŠ è½½ONNXæ¨¡å‹
with open(onnx_model_path, "rb") as model_file:
    if not parser.parse(model_file.read()):
        print("ONNXæ¨¡å‹è§£æå¤±è´¥")
        for error in range(parser.num_errors):
            print(parser.get_error(error))
        exit()

print("ONNXæ¨¡å‹å·²æˆåŠŸåŠ è½½è‡³TensorRTç½‘ç»œ")

# æ„å»ºTensorRTå¼•æ“
builder_config = builder.create_builder_config()
builder_config.max_workspace_size = 1 << 30  # æœ€å¤§å·¥ä½œç©ºé—´è®¾ç½®ä¸º1GB

# å¯ç”¨FP16ç²¾åº¦
if builder.platform_has_fast_fp16:
    builder_config.set_flag(trt.BuilderFlag.FP16)

# æ„å»ºå¼•æ“
engine = builder.build_engine(network, builder_config)
with open(trt_engine_path, "wb") as engine_file:
    engine_file.write(engine.serialize())

print(f"TensorRTå¼•æ“å·²ä¿å­˜è‡³ {trt_engine_path}")
åœ¨TensorRTä¸­è¿›è¡ŒINT8é‡åŒ–éœ€è¦æ ¡å‡†æ•°æ®é›†ã€‚ä»¥ä¸‹ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ ¡å‡†å™¨å’Œæ ¡å‡†æ•°æ®è¿›è¡Œé‡åŒ–ï¼š
import os
import random

class BertCalibrator(trt.IInt8EntropyCalibrator2):
    def __init__(self, calibration_data, batch_size=8, max_length=128):
        trt.IInt8EntropyCalibrator2.__init__(self)
        self.calibration_data = calibration_data
        self.batch_size = batch_size
        self.max_length = max_length
        self.device_input = cuda.mem_alloc(batch_size * max_length * np.dtype(np.int32).itemsize)
        self.current_index = 0

    def get_batch_size(self):
        return self.batch_size

    def get_batch(self, names):
        if self.current_index + self.batch_size > len(self.calibration_data):
            return None

        batch = self.calibration_data[self.current_index:self.current_index + self.batch_size]
        self.current_index += self.batch_size

        cuda.memcpy_htod(self.device_input, np.ascontiguousarray(batch))
        return [int(self.device_input)]

    def read_calibration_cache(self):
        return None

    def write_calibration_cache(self, cache):
        pass

# ç¤ºä¾‹æ ¡å‡†æ•°æ®ï¼ˆéšæœºç”Ÿæˆï¼Œç”¨å®é™…æ•°æ®æ›¿ä»£ï¼‰
calibration_data = np.random.randint(0, 10000, size=(100, 128)).astype(np.int32)

# æ„å»ºé‡åŒ–å¼•æ“
builder_config.set_flag(trt.BuilderFlag.INT8)
calibrator = BertCalibrator(calibration_data)
builder_config.int8_calibrator = calibrator
int8_engine = builder.build_engine(network, builder_config)

# ä¿å­˜INT8å¼•æ“
with open("bert_sentiment_int8.trt", "wb") as int8_engine_file:
    int8_engine_file.write(int8_engine.serialize())

print("INT8å¼•æ“å·²æˆåŠŸç”Ÿæˆ")
åŠ è½½TensorRTå¼•æ“ï¼Œå¹¶å¯¹ç¤ºä¾‹è¾“å…¥è¿›è¡Œæ¨ç†ï¼ŒéªŒè¯æ¨ç†åŠ é€Ÿæ•ˆæœï¼š
# åŠ è½½å¼•æ“
def load_engine(trt_runtime, engine_path):
    with open(engine_path, "rb") as f:
        engine_data = f.read()
    return trt_runtime.deserialize_cuda_engine(engine_data)

# åˆ›å»ºä¸Šä¸‹æ–‡
runtime = trt.Runtime(logger)
engine = load_engine(runtime, trt_engine_path)
context = engine.create_execution_context()

# åˆ†é…å†…å­˜
input_shape = (1, 128)
output_shape = (1, 2)

d_input = cuda.mem_alloc(np.prod(input_shape) * np.dtype(np.float32).itemsize)
d_output = cuda.mem_alloc(np.prod(output_shape) * np.dtype(np.float32).itemsize)

# è¾“å…¥è¾“å‡ºç»‘å®š
bindings = [int(d_input), int(d_output)]

# æ¨ç†æ•°æ®
input_data = np.random.rand(*input_shape).astype(np.float32)
cuda.memcpy_htod(d_input, input_data)

# æ¨ç†
context.execute_v2(bindings)

# è·å–è¾“å‡º
output_data = np.empty(output_shape, dtype=np.float32)
cuda.memcpy_dtoh(output_data, d_output)
print("æ¨ç†è¾“å‡º:", output_data)
æœ€ç»ˆè¿è¡Œç»“æœå¦‚ä¸‹ï¼š
# åŠ è½½å¼•æ“
def load_engine(trt_runtime, engine_path):
    with open(engine_path, "rb") as f:
        engine_data = f.read()
    return trt_runtime.deserialize_cuda_engine(engine_data)

# åˆ›å»ºä¸Šä¸‹æ–‡
runtime = trt.Runtime(logger)
engine = load_engine(runtime, trt_engine_path)
context = engine.create_execution_context()

# åˆ†é…å†…å­˜
input_shape = (1, 128)
output_shape = (1, 2)

d_input = cuda.mem_alloc(np.prod(input_shape) * np.dtype(np.float32).itemsize)
d_output = cuda.mem_alloc(np.prod(output_shape) * np.dtype(np.float32).itemsize)

# è¾“å…¥è¾“å‡ºç»‘å®š
bindings = [int(d_input), int(d_output)]

# æ¨ç†æ•°æ®
input_data = np.random.rand(*input_shape).astype(np.float32)
cuda.memcpy_htod(d_input, input_data)

# æ¨ç†
context.execute_v2(bindings)

# è·å–è¾“å‡º
output_data = np.empty(output_shape, dtype=np.float32)
cuda.memcpy_dtoh(output_data, d_output)
print("æ¨ç†è¾“å‡º:", output_data)

# ä¾‹12-9
import onnxruntime as ort
import numpy as np

# å®šä¹‰ ONNX æ¨¡å‹è·¯å¾„
onnx_model_path = "bert_sentiment.onnx"

# é…ç½®å¤šçº¿ç¨‹æ¨ç†é€‰é¡¹
sess_options = ort.SessionOptions()
sess_options.intra_op_num_threads = 4  # è®¾ç½®ä¸º 4 ä¸ªçº¿ç¨‹
sess_options.inter_op_num_threads = 2  # è®¾ç½®å¹¶å‘è®¡ç®—çš„çº¿ç¨‹æ•°
sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL  # å¯ç”¨å¹¶è¡Œæ¨¡å¼
sess_options.log_severity_level = 3  # é™ä½æ—¥å¿—è¾“å‡ºçº§åˆ«

# åŠ è½½ ONNX Runtime æ¨ç†ä¼šè¯
session = ort.InferenceSession(onnx_model_path, sess_options)

print("ONNX Runtime æ¨ç†ä¼šè¯å·²æˆåŠŸåŠ è½½")
ä½¿ç”¨å¤šçº¿ç¨‹è¿›è¡Œæ¨ç†ï¼Œå¯ä»¥åœ¨å•æœºä¸Šå¤„ç†å¤šä¸ªè¾“å…¥ä»»åŠ¡ï¼Œä»¥æé«˜æ¨¡å‹çš„ååé‡ï¼š
# ç¤ºä¾‹è¾“å…¥æ•°æ®
batch_size = 8
sequence_length = 128
input_data = np.random.randint(0, 10000, (batch_size, sequence_length)).astype(np.int64)
attention_data = np.ones((batch_size, sequence_length)).astype(np.int64)

# å®šä¹‰è¾“å…¥å­—å…¸
onnx_inputs = {
    "input_ids": input_data,
    "attention_mask": attention_data
}

# å¤šçº¿ç¨‹æ¨ç†
outputs = session.run(None, onnx_inputs)
logits = outputs[0]

# æ˜¾ç¤ºæ¨ç†ç»“æœ
print("ONNX Runtime æ¨ç†è¾“å‡º:", logits)
åœ¨æœ¬åœ°é€šè¿‡å¤šè¿›ç¨‹æ¨¡æ‹Ÿåˆ†å¸ƒå¼æ¨ç†åœºæ™¯ï¼Œå¯ä»¥ä½¿ç”¨Pythonçš„multiprocessingåº“ï¼š
from multiprocessing import Process, Queue

def onnx_worker(input_data, output_queue):
    # å•ç‹¬åŠ è½½ä¸€ä¸ª ONNX Runtime ä¼šè¯
    local_session = ort.InferenceSession(onnx_model_path)
    # æ¨ç†
    outputs = local_session.run(None, input_data)
    output_queue.put(outputs[0])  # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—

# åˆ›å»ºè¿›ç¨‹é˜Ÿåˆ—
output_queue = Queue()

# åˆ›å»ºç¤ºä¾‹ä»»åŠ¡æ•°æ®
task_1 = {"input_ids": np.random.randint(0, 10000, (1, sequence_length)).astype(np.int64),
          "attention_mask": np.ones((1, sequence_length)).astype(np.int64)}

task_2 = {"input_ids": np.random.randint(0, 10000, (1, sequence_length)).astype(np.int64),
          "attention_mask": np.ones((1, sequence_length)).astype(np.int64)}

# å¯åŠ¨ä¸¤ä¸ªè¿›ç¨‹è¿›è¡Œæ¨ç†
process_1 = Process(target=onnx_worker, args=(task_1, output_queue))
process_2 = Process(target=onnx_worker, args=(task_2, output_queue))

process_1.start()
process_2.start()

process_1.join()
process_2.join()

# è·å–ç»“æœ
result_1 = output_queue.get()
result_2 = output_queue.get()

print("åˆ†å¸ƒå¼æ¨ç†ç»“æœä»»åŠ¡1:", result_1)
print("åˆ†å¸ƒå¼æ¨ç†ç»“æœä»»åŠ¡2:", result_2)



# ä¾‹12-10
import tensorrt as trt
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit

# å®šä¹‰ONNXæ¨¡å‹è·¯å¾„
onnx_model_path = "bert_sentiment.onnx"

# TensorRTæ—¥å¿—è®°å½•å™¨
logger = trt.Logger(trt.Logger.WARNING)

# åˆ›å»ºæ„å»ºå™¨ã€ç½‘ç»œå®šä¹‰å’Œè§£æå™¨
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, logger)

# åŠ è½½ONNXæ¨¡å‹
with open(onnx_model_path, "rb") as model_file:
    if not parser.parse(model_file.read()):
        print("ONNXæ¨¡å‹è§£æå¤±è´¥")
        for error in range(parser.num_errors):
            print(parser.get_error(error))
        exit()

print("ONNXæ¨¡å‹åŠ è½½å®Œæˆ")

# é…ç½®åŠ¨æ€æ‰¹é‡å¤§å°
builder_config = builder.create_builder_config()
builder_config.max_workspace_size = 1 << 30  # è®¾ç½®æœ€å¤§å·¥ä½œç©ºé—´ä¸º1GB

# è®¾ç½®åŠ¨æ€æ‰¹é‡å¤§å°èŒƒå›´
profile = builder.create_optimization_profile()
input_name = network.get_input(0).name
profile.set_shape(input_name, (1, 128), (4, 128), (16, 128))  # æœ€å°ã€æœ€ä¼˜å’Œæœ€å¤§æ‰¹é‡å¤§å°
builder_config.add_optimization_profile(profile)

# æ„å»ºå¼•æ“
engine = builder.build_engine(network, builder_config)
print("TensorRTå¼•æ“æ„å»ºå®Œæˆ")
å®ç°ä¸€ä¸ªç®€å•çš„è‡ªå®šä¹‰ç®—å­ï¼Œå‡è®¾éœ€è¦å¯¹logitså€¼æ‰§è¡Œè‡ªå®šä¹‰æ“ä½œï¼ˆå¦‚æ·»åŠ åç½®ï¼‰ï¼š
import ctypes

# å®šä¹‰è‡ªå®šä¹‰ç®—å­çš„åŠ¨æ€åº“è·¯å¾„
custom_plugin_path = "./custom_plugin.so"

# åŠ è½½è‡ªå®šä¹‰ç®—å­æ’ä»¶
ctypes.CDLL(custom_plugin_path)
print("è‡ªå®šä¹‰ç®—å­æ’ä»¶åŠ è½½æˆåŠŸ")

# åˆ›å»ºæ’ä»¶æ³¨å†Œå™¨
plugin_registry = trt.get_plugin_registry()
plugin_creator = plugin_registry.get_plugin_creator("CustomOp", "1", "")

# è®¾ç½®è‡ªå®šä¹‰ç®—å­å‚æ•°
plugin_fields = trt.PluginFieldCollection([
    trt.PluginField("bias", np.array([0.5], dtype=np.float32))
])

custom_plugin = plugin_creator.create_plugin("custom_op", plugin_fields)

# æ·»åŠ è‡ªå®šä¹‰ç®—å­åˆ°ç½‘ç»œ
input_tensor = network.get_input(0)
custom_layer = network.add_plugin_v2([input_tensor], custom_plugin)
network.mark_output(custom_layer.get_output(0))

# æ„å»ºå¸¦æœ‰è‡ªå®šä¹‰ç®—å­çš„å¼•æ“
engine_with_custom_op = builder.build_engine(network, builder_config)
print("å¸¦æœ‰è‡ªå®šä¹‰ç®—å­çš„TensorRTå¼•æ“æ„å»ºå®Œæˆ")
é€šè¿‡åŠ è½½æ”¯æŒåŠ¨æ€æ‰¹é‡å¤§å°çš„å¼•æ“ï¼Œæ‰§è¡Œæ¨ç†ä»»åŠ¡ï¼š
# åŠ è½½å¼•æ“
def load_engine(trt_runtime, engine_path):
    with open(engine_path, "rb") as f:
        engine_data = f.read()
    return trt_runtime.deserialize_cuda_engine(engine_data)

runtime = trt.Runtime(logger)
engine = load_engine(runtime, "bert_sentiment_dynamic.trt")
context = engine.create_execution_context()

# è®¾ç½®åŠ¨æ€æ‰¹é‡å¤§å°
batch_size = 8
context.set_binding_shape(0, (batch_size, 128))

# åˆ†é…å†…å­˜
input_shape = (batch_size, 128)
output_shape = (batch_size, 2)

d_input = cuda.mem_alloc(np.prod(input_shape) * np.dtype(np.float32).itemsize)
d_output = cuda.mem_alloc(np.prod(output_shape) * np.dtype(np.float32).itemsize)

# è¾“å…¥è¾“å‡ºç»‘å®š
bindings = [int(d_input), int(d_output)]

# å‡†å¤‡è¾“å…¥æ•°æ®
input_data = np.random.rand(*input_shape).astype(np.float32)
cuda.memcpy_htod(d_input, input_data)

# æ¨ç†
context.execute_v2(bindings)

# è·å–è¾“å‡º
output_data = np.empty(output_shape, dtype=np.float32)
cuda.memcpy_dtoh(output_data, d_output)
print("åŠ¨æ€æ‰¹é‡å¤§å°æ¨ç†è¾“å‡º:", output_data)




# ä¾‹12-11
import pandas as pd
import re

# ç¤ºä¾‹é—®ç­”æ•°æ®
data = {
    "question": [
        "How to reset my password?",
        "how to reset my password?",
        "What is your refund policy?",
        "What  is your refund policy?   ",
        "I love your service! ğŸ˜"
    ],
    "answer": [
        "Please follow the steps on our website.",
        "Please follow the steps on our website.",
        "You can find details on our refund policy page.",
        "You can find details on our refund policy page.",
        "Thank you! We're glad to hear that."
    ]
}

# åŠ è½½æ•°æ®ä¸ºDataFrame
df = pd.DataFrame(data)

# æ•°æ®æ¸…æ´—å‡½æ•°
def clean_text(text):
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r"\s+", " ", text.strip())
    # å»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    text = re.sub(r"[^\w\s.,!?]", "", text)
    # è½¬ä¸ºå°å†™
    text = text.lower()
    return text

def clean_data(df):
    # æ¸…æ´—é—®é¢˜å’Œç­”æ¡ˆ
    df["question"] = df["question"].apply(clean_text)
    df["answer"] = df["answer"].apply(clean_text)
    # å»é‡
    df = df.drop_duplicates(subset=["question", "answer"]).reset_index(drop=True)
    return df

# åº”ç”¨æ¸…æ´—å‡½æ•°
df_cleaned = clean_data(df)
print("æ¸…æ´—åçš„æ•°æ®:\n", df_cleaned)
è¿è¡Œç»“æœå¦‚ä¸‹ï¼š
æ¸…æ´—åçš„æ•°æ®:
                     question                                         answer
0  how to reset my password?    please follow the steps on our website.
1  what is your refund policy?  you can find details on our refund policy page.
2  i love your service!                 thank you were glad to hear that.



# ä¾‹12-12
from nltk.corpus import wordnet
import random

# åŒä¹‰è¯æ›¿æ¢
def synonym_replacement(sentence):
    words = sentence.split()
    new_sentence = []
    for word in words:
        synonyms = wordnet.synsets(word)
        if synonyms:
            synonym = synonyms[0].lemmas()[0].name()
            new_sentence.append(synonym if random.random() > 0.7 else word)
        else:
            new_sentence.append(word)
    return " ".join(new_sentence)

# éšæœºæ’å…¥
def random_insertion(sentence, insert_words):
    words = sentence.split()
    for _ in range(2):  # æ’å…¥ä¸¤æ¬¡
        idx = random.randint(0, len(words))
        words.insert(idx, random.choice(insert_words))
    return " ".join(words)

# ç¤ºä¾‹æ•°æ®å¢å¼º
question = "how to reset my password?"
insert_words = ["please", "help", "guide"]
augmented_question_1 = synonym_replacement(question)
augmented_question_2 = random_insertion(question, insert_words)

print("åŸå§‹é—®é¢˜:", question)
print("åŒä¹‰è¯æ›¿æ¢:", augmented_question_1)
print("éšæœºæ’å…¥:", augmented_question_2)
è¿è¡Œç»“æœå¦‚ä¸‹ï¼š
åŸå§‹é—®é¢˜: how to reset my password?
åŒä¹‰è¯æ›¿æ¢: how to reset my parole?
éšæœºæ’å…¥: how to reset please my help password?
3.æ•°æ®æ ¼å¼åŒ–
æ ¼å¼åŒ–æ•°æ®ä¸ºæ¨¡å‹å¯æ¥å—çš„è¾“å…¥å½¢å¼ï¼Œå¦‚JSONæˆ–CSVæ–‡ä»¶ã€‚å¸¸è§æ ¼å¼å¦‚ä¸‹ï¼š
ï¼ˆ1ï¼‰JSONæ ¼å¼ï¼š
[
    {
        "question": "how to reset my password?",
        "answer": "please follow the steps on our website."
    },
    {
        "question": "what is your refund policy?",
        "answer": "you can find details on our refund policy page."
    }
]
å…·ä½“ä»£ç å®ç°ï¼š
import json

# å°†æ¸…æ´—åçš„æ•°æ®è½¬æ¢ä¸ºJSONæ ¼å¼
def format_to_json(df, output_path):
    records = df.to_dict(orient="records")
    with open(output_path, "w") as f:
        json.dump(records, f, indent=4)

# ä¿å­˜ä¸ºJSONæ–‡ä»¶
format_to_json(df_cleaned, "cleaned_data.json")
print("æ•°æ®å·²æ ¼å¼åŒ–ä¸ºJSONæ–‡ä»¶")
ä»¥ä¸‹æ˜¯ç»¼åˆä¸Šè¿°æ­¥éª¤çš„å®Œæ•´å®ä¾‹ï¼Œç»“åˆä¼ä¸šé—®ç­”çš„å…·ä½“åœºæ™¯ï¼ˆå¦‚å¯†ç é‡ç½®ã€é€€æ¬¾æ”¿ç­–ã€è´¦æˆ·ç®¡ç†ç­‰ï¼‰ï¼Œå±•ç¤ºå¦‚ä½•æ¸…æ´—ã€å¢å¼ºå’Œæ ¼å¼åŒ–é—®ç­”æ•°æ®ã€‚æµ‹è¯•éƒ¨åˆ†æ¨¡æ‹Ÿäº†å¤§é‡ä¼ä¸šé—®ç­”æƒ…æ™¯ï¼Œå¹¶è¾“å‡ºæ¸…æ´—å’Œå¢å¼ºåçš„ç»“æœã€‚
import pandas as pd
import re
import random
import json

# ç¤ºä¾‹ä¼ä¸šé—®ç­”æ•°æ®ï¼ˆä¸­æ–‡åœºæ™¯ï¼‰
data = {
    "question": [
        "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
        "æˆ‘æ€æ ·æ›´æ”¹è´¦æˆ·é‚®ç®±åœ°å€ï¼Ÿ",
        "è´µå…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•è”ç³»å®¢æˆ·æ”¯æŒï¼Ÿ",
        "æœ‰å“ªäº›è®¢é˜…è®¡åˆ’å¯ä»¥é€‰æ‹©ï¼Ÿ"
    ],
    "answer": [
        "æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚",
        "è¯·å‰å¾€è´¦æˆ·è®¾ç½®æ›´æ”¹é‚®ç®±åœ°å€ã€‚",
        "æˆ‘ä»¬çš„é€€æ¬¾æ”¿ç­–è¯¦è§å¸¸è§é—®é¢˜é¡µé¢ã€‚",
        "æ‚¨å¯ä»¥é€šè¿‡èŠå¤©æˆ–é‚®ä»¶è”ç³»å®¢æˆ·æ”¯æŒã€‚",
        "æˆ‘ä»¬æä¾›æœˆåº¦å’Œå¹´åº¦è®¢é˜…è®¡åˆ’ã€‚"
    ]
}

# åŠ è½½æ•°æ®ä¸ºDataFrame
df = pd.DataFrame(data)

# æ¸…æ´—å‡½æ•°
def clean_text(text):
    text = re.sub(r"\s+", "", text.strip())  # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9.,!?ï¼Œã€‚ï¼ï¼Ÿ]", "", text)  # å»é™¤ç‰¹æ®Šå­—ç¬¦
    return text

def clean_data(df):
    df["question"] = df["question"].apply(clean_text)
    df["answer"] = df["answer"].apply(clean_text)
    df = df.drop_duplicates(subset=["question", "answer"]).reset_index(drop=True)
    return df

# æ•°æ®å¢å¼ºå‡½æ•°
def synonym_replacement(sentence, synonyms_dict):
    words = list(sentence)
    new_sentence = []
    for word in words:
        if word in synonyms_dict and random.random() > 0.7:
            new_sentence.append(synonyms_dict[word])
        else:
            new_sentence.append(word)
    return "".join(new_sentence)

def random_insertion(sentence, insert_words):
    words = list(sentence)
    for _ in range(2):  # æ’å…¥ä¸¤æ¬¡
        idx = random.randint(0, len(words))
        words.insert(idx, random.choice(insert_words))
    return "".join(words)

# ç¤ºä¾‹åŒä¹‰è¯æ›¿æ¢è¯å…¸å’Œæ’å…¥è¯
synonyms_dict = {"é‡ç½®": "é‡æ–°è®¾ç½®", "å®¢æˆ·": "ç”¨æˆ·", "æ”¯æŒ": "å¸®åŠ©"}
insert_words = ["è¯·", "è°¢è°¢", "æŒ‡å¯¼"]

# æ•°æ®å¢å¼ºä¸»å‡½æ•°
def augment_data(df, synonyms_dict, insert_words):
    augmented_questions = []
    for question in df["question"]:
        augmented_questions.append(synonym_replacement(question, synonyms_dict))
        augmented_questions.append(random_insertion(question, insert_words))
    return augmented_questions

# æ•°æ®æ ¼å¼åŒ–å‡½æ•°
def format_to_json(df, augmented_questions, output_path):
    records = df.to_dict(orient="records")
    for question in augmented_questions:
        records.append({"question": question, "answer": "åŒåŸå§‹é—®é¢˜ç­”æ¡ˆä¸€è‡´"})
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(records, f, indent=4, ensure_ascii=False)

# æ¸…æ´—æ•°æ®
df_cleaned = clean_data(df)
print("æ¸…æ´—åçš„æ•°æ®:\n", df_cleaned)

# æ•°æ®å¢å¼º
augmented_questions = augment_data(df_cleaned, synonyms_dict, insert_words)
print("å¢å¼ºåçš„é—®é¢˜ç¤ºä¾‹:\n", augmented_questions[:5])

# æ ¼å¼åŒ–ä¸ºJSONæ–‡ä»¶
format_to_json(df_cleaned, augmented_questions, "cleaned_data.json")
print("æ•°æ®å·²æ ¼å¼åŒ–ä¸ºJSONæ–‡ä»¶")
æ¸…æ´—åçš„æ•°æ®ç¤ºä¾‹ï¼š
æ¸…æ´—åçš„æ•°æ®:
                 question              answer
0            å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ         æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚
1      æˆ‘æ€æ ·æ›´æ”¹è´¦æˆ·é‚®ç®±åœ°å€ï¼Ÿ       è¯·å‰å¾€è´¦æˆ·è®¾ç½®æ›´æ”¹é‚®ç®±åœ°å€ã€‚
2      è´µå…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ   æˆ‘ä»¬çš„é€€æ¬¾æ”¿ç­–è¯¦è§å¸¸è§é—®é¢˜é¡µé¢ã€‚
3         å¦‚ä½•è”ç³»å®¢æˆ·æ”¯æŒï¼Ÿ    æ‚¨å¯ä»¥é€šè¿‡èŠå¤©æˆ–é‚®ä»¶è”ç³»å®¢æˆ·æ”¯æŒã€‚
4    æœ‰å“ªäº›è®¢é˜…è®¡åˆ’å¯ä»¥é€‰æ‹©ï¼Ÿ       æˆ‘ä»¬æä¾›æœˆåº¦å’Œå¹´åº¦è®¢é˜…è®¡åˆ’ã€‚
æ•°æ®å¢å¼ºç¤ºä¾‹è¾“å‡ºï¼š
å¢å¼ºåçš„é—®é¢˜ç¤ºä¾‹:
 ['å¦‚ä½•é‡æ–°è®¾ç½®å¯†ç ï¼Ÿ', 'è¯·å¦‚ä½•è°¢è°¢é‡ç½®å¯†ç ï¼Ÿ', 'æˆ‘æ€æ ·æ›´æ”¹è´¦æˆ·é‚®ç®±åœ°å€ï¼Ÿ', 'è¯·æˆ‘æ€æ ·æ›´æ”¹è°¢è°¢è´¦æˆ·é‚®ç®±åœ°å€ï¼Ÿ', 'è´µå…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ']
JSONæ–‡ä»¶å†…å®¹ç¤ºä¾‹ï¼š
[
    {
        "question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
        "answer": "æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚"
    },
    {
        "question": "æˆ‘æ€æ ·æ›´æ”¹è´¦æˆ·é‚®ç®±åœ°å€ï¼Ÿ",
        "answer": "è¯·å‰å¾€è´¦æˆ·è®¾ç½®æ›´æ”¹é‚®ç®±åœ°å€ã€‚"
    },
    {
        "question": "å¦‚ä½•é‡æ–°è®¾ç½®å¯†ç ï¼Ÿ",
        "answer": "åŒåŸå§‹é—®é¢˜ç­”æ¡ˆä¸€è‡´"
    },
    {
        "question": "è¯·å¦‚ä½•è°¢è°¢é‡ç½®å¯†ç ï¼Ÿ",
        "answer": "åŒåŸå§‹é—®é¢˜ç­”æ¡ˆä¸€è‡´"


    }


# ä¾‹12-13
pip install transformers datasets torch
å°†ç»è¿‡æ¸…æ´—å’Œå¢å¼ºçš„ä¼ä¸šé—®ç­”æ•°æ®åŠ è½½ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†ï¼š
from datasets import Dataset
from transformers import AutoTokenizer

# åŠ è½½æ•°æ®
data = [
    {"question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ", "answer": "æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚", "label": 1},
    {"question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ", "answer": "è¯·å‰å¾€è´¦æˆ·è®¾ç½®æ›´æ”¹é‚®ç®±åœ°å€ã€‚", "label": 0},
    {"question": "å¦‚ä½•è”ç³»å®¢æˆ·æ”¯æŒï¼Ÿ", "answer": "æ‚¨å¯ä»¥é€šè¿‡èŠå¤©æˆ–é‚®ä»¶è”ç³»å®¢æˆ·æ”¯æŒã€‚", "label": 1},
    {"question": "è´µå…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "æˆ‘ä»¬çš„é€€æ¬¾æ”¿ç­–è¯¦è§å¸¸è§é—®é¢˜é¡µé¢ã€‚", "label": 1},
    {"question": "è´µå…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "æ‚¨å¯ä»¥é€šè¿‡èŠå¤©æˆ–é‚®ä»¶è”ç³»å®¢æˆ·æ”¯æŒã€‚", "label": 0}
]

dataset = Dataset.from_list(data)

# åˆå§‹åŒ–åˆ†è¯å™¨
model_name = "bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# æ•°æ®å¤„ç†å‡½æ•°
def preprocess(example):
    encoded = tokenizer(
        example["question"],
        example["answer"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    encoded["label"] = example["label"]
    return encoded

# å¤„ç†æ•°æ®é›†
processed_dataset = dataset.map(preprocess, batched=True)
train_test_split = processed_dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split["train"]
val_dataset = train_test_split["test"]

print("æ ·æœ¬æ•°æ®:", train_dataset[0])
ä½¿ç”¨transformersåº“ä¸­çš„AutoModelForSequenceClassificationï¼Œå¯¹é—®ç­”åŒ¹é…ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼š
import torch
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./model_output",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2
)

# å®šä¹‰Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
è®­ç»ƒå®Œæˆåï¼Œéœ€è¦å°†æ¨¡å‹éƒ¨ç½²ä¸ºæ¨ç†æœåŠ¡ï¼Œä»¥æ”¯æŒå®æ—¶é—®ç­”è¯·æ±‚ï¼Œå°†å¾®è°ƒåçš„æ¨¡å‹ä¿å­˜ä¸ºå¯éƒ¨ç½²çš„æ ¼å¼ï¼š
model.save_pretrained("./deployed_model")
tokenizer.save_pretrained("./deployed_model")
ä½¿ç”¨FastAPIæ„å»ºRESTful APIæ¥å£ï¼Œæä¾›é—®ç­”æ¨ç†æœåŠ¡ï¼š
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI()

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_path = "./deployed_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.eval()

# å®šä¹‰è¯·æ±‚å’Œå“åº”æ¨¡å‹
class QARequest(BaseModel):
    question: str
    answer: str

@app.post("/predict/")
def predict(data: QARequest):
    inputs = tokenizer(
        data.question, data.answer,
        truncation=True, padding="max_length",
        max_length=128,
        return_tensors="pt"
    )
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        prediction = torch.argmax(probs).item()
    return {"prediction": prediction, "probabilities": probs.tolist()}
è¿è¡ŒæœåŠ¡ï¼š
uvicorn app:app --host 0.0.0.0 --port 8000
é€šè¿‡å‘é€æµ‹è¯•è¯·æ±‚ï¼ŒéªŒè¯æ¨ç†æœåŠ¡çš„æ­£ç¡®æ€§ï¼š
import requests
url = "http://127.0.0.1:8000/predict/"
data = {"question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ", "answer": "æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚"}
response = requests.post(url, json=data)
print(response.json())
è¿è¡Œç»“æœå¦‚ä¸‹ï¼š
{"prediction": 1, "probabilities": [[0.1, 0.9]]}


# ä¾‹12-14
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI()

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_path = "./deployed_model"  # æ›¿æ¢ä¸ºå¾®è°ƒåçš„æ¨¡å‹è·¯å¾„
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.eval()

# å®šä¹‰è¯·æ±‚æ•°æ®ç»“æ„
class QARequest(BaseModel):
    question: str
    answer: str

# å®šä¹‰APIæ¥å£
@app.post("/qa/")
def predict(data: QARequest):
    """
    æ¥æ”¶ç”¨æˆ·è¯·æ±‚æ•°æ®ï¼Œè¿”å›é—®ç­”åŒ¹é…çš„é¢„æµ‹ç»“æœ
    """
    # å¯¹è¯·æ±‚ä¸­çš„é—®é¢˜å’Œç­”æ¡ˆè¿›è¡Œåˆ†è¯å¤„ç†
    inputs = tokenizer(
        data.question,
        data.answer,
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )
    # æ¨¡å‹æ¨ç†
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        prediction = torch.argmax(probs, dim=-1).item()
    
    # è¿”å›é¢„æµ‹ç»“æœ
    return {
        "question": data.question,
        "answer": data.answer,
        "prediction": prediction,  # 1ä¸ºåŒ¹é…ï¼Œ0ä¸ºä¸åŒ¹é…
        "probabilities": probs.tolist()  # å„ç±»åˆ«æ¦‚ç‡
    }
å°†APIæœåŠ¡è¿è¡Œåœ¨æœ¬åœ°æˆ–æœåŠ¡å™¨ä¸Šï¼š
uvicorn app:app --host 0.0.0.0 --port 8000
é€šè¿‡HTTPè¯·æ±‚æµ‹è¯•æ¥å£çš„åŠŸèƒ½å’Œæ­£ç¡®æ€§ï¼Œä»¥ä¸‹æä¾›Pythonä»£ç ç¤ºä¾‹ï¼š
import requests

# å®šä¹‰æµ‹è¯•æ•°æ®
url = "http://127.0.0.1:8000/qa/"
data = {
    "question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
    "answer": "æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚"
}

# å‘é€POSTè¯·æ±‚
response = requests.post(url, json=data)

# è¾“å‡ºå“åº”ç»“æœ
print("APIå“åº”æ•°æ®:", response.json())
ä»¥ä¸‹æ˜¯å‘é€æµ‹è¯•è¯·æ±‚åçš„ç¤ºä¾‹å“åº”ç»“æœï¼š
{
    "question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
    "answer": "æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚",
    "prediction": 1,
    "probabilities": [[0.1, 0.9]]
}


# ä¾‹12-15
pip install loguru
ä½¿ç”¨loguruåº“è®°å½•ç³»ç»Ÿè¿è¡Œæ—¥å¿—ï¼ŒåŒ…æ‹¬è¯·æ±‚æ—¥å¿—ã€å“åº”æ—¶é—´å’Œå¼‚å¸¸ä¿¡æ¯ï¼š
from fastapi import FastAPI, Request
from loguru import logger
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import time

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI()

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_path = "./deployed_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.eval()

# é…ç½®æ—¥å¿—æ–‡ä»¶
logger.add("logs/system.log", rotation="1 MB", retention="7 days", level="INFO")

# å®šä¹‰è¯·æ±‚æ•°æ®æ¨¡å‹
class QARequest(BaseModel):
    question: str
    answer: str

# å®šä¹‰APIæ¥å£
@app.post("/qa/")
async def predict(data: QARequest, request: Request):
    """
    æ¥æ”¶é—®ç­”è¯·æ±‚ï¼Œè¿”å›é¢„æµ‹ç»“æœï¼ŒåŒæ—¶è®°å½•è¯·æ±‚ä¸å“åº”æ—¥å¿—
    """
    start_time = time.time()
    client_ip = request.client.host

    # åˆ†è¯ä¸æ¨¡å‹æ¨ç†
    try:
        inputs = tokenizer(
            data.question,
            data.answer,
            truncation=True,
            padding="max_length",
            max_length=128,
            return_tensors="pt"
        )
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            prediction = torch.argmax(probs, dim=-1).item()
        
        # è®°å½•æˆåŠŸæ—¥å¿—
        response_time = time.time() - start_time
        logger.info(
            f"Client IP: {client_ip}, Question: {data.question}, "
            f"Answer: {data.answer}, Prediction: {prediction}, "
            f"Response Time: {response_time:.4f}s"
        )

        return {
            "prediction": prediction,
            "probabilities": probs.tolist(),
            "response_time": f"{response_time:.4f}s"
        }
    except Exception as e:
        # è®°å½•å¼‚å¸¸æ—¥å¿—
        logger.error(f"Error processing request from {client_ip}: {str(e)}")
        return {"error": "An error occurred while processing your request."}
æ€§èƒ½ç›‘æ§å¯ä»¥å¸®åŠ©å¼€å‘è€…è¯„ä¼°ç³»ç»Ÿè´Ÿè½½å’Œå“åº”èƒ½åŠ›ï¼Œå¹¶åŠæ—¶ä¼˜åŒ–ï¼ŒPrometheusæ˜¯å¼€æºçš„æ€§èƒ½ç›‘æ§å·¥å…·ï¼Œå¯ä»¥é€šè¿‡å…¶Pythonå®¢æˆ·ç«¯é‡‡é›†FastAPIæœåŠ¡çš„æŒ‡æ ‡ã€‚
é¦–å…ˆå®‰è£…Prometheuså®¢æˆ·ç«¯ï¼š
pip install prometheus-client
é›†æˆPrometheusæŒ‡æ ‡ï¼Œä»¥ä¸‹ä»£ç å±•ç¤ºäº†å¦‚ä½•è®°å½•è¯·æ±‚è®¡æ•°å’Œå“åº”æ—¶é—´ï¼š
from prometheus_client import Counter, Histogram, start_http_server

# å¯åŠ¨Prometheusç›‘æ§æœåŠ¡
start_http_server(8001)

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter("request_count", "Total number of requests")
RESPONSE_TIME = Histogram("response_time", "Response time of requests")

@app.post("/qa/")
@RESPONSE_TIME.time()
async def predict_with_metrics(data: QARequest, request: Request):
    """
    å¸¦æœ‰PrometheusæŒ‡æ ‡çš„é¢„æµ‹æ¥å£
    """
    REQUEST_COUNT.inc()  # å¢åŠ è¯·æ±‚è®¡æ•°
    response = await predict(data, request)
    return response
æ­¤å¤–ï¼Œæ—¥å¿—æ–‡ä»¶ä¸­åŒ…å«æ‰€æœ‰è¯·æ±‚çš„å¼‚å¸¸ä¿¡æ¯ï¼Œå¯ä»¥é€šè¿‡å®šæœŸåˆ†ææ—¥å¿—å¿«é€Ÿå®šä½é—®é¢˜ï¼Œç»“åˆPrometheus Alertmanagerå®ç°å¼‚å¸¸å‘Šè­¦ï¼Œé…ç½®è§„åˆ™å¦‚ä¸‹ï¼š
groups:
  - name: alert_rules
    rules:
    - alert: HighErrorRate
      expr: rate(request_errors_total[5m]) > 0.1
      for: 1m
      labels:
        severity: warning
      annotations:
        summary: "High error rate detected"
        description: "More than 10% of requests failed in the last 5 minutes"
æ—¥å¿—æ–‡ä»¶å†…å®¹ç¤ºä¾‹ï¼š
2024-11-17 10:30:15.123 | INFO     | Client IP: 127.0.0.1, Question: å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ, Answer: æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚, Prediction: 1, Response Time: 0.1234s
2024-11-17 10:30:20.456 | ERROR    | Error processing request from 127.0.0.1: tokenizer input length exceeded maximum
ç»¼åˆæµ‹è¯•ä»£ç å¦‚ä¸‹ï¼š
import requests
# å®šä¹‰API URL
url = "http://127.0.0.1:8000/qa/"
# æµ‹è¯•æ•°æ®
test_data = [
    {"question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ", "answer": "æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚"},  # æ­£ç¡®åŒ¹é…
    {"question": "å¦‚ä½•æ›´æ”¹è´¦æˆ·é‚®ç®±ï¼Ÿ", "answer": "è¯·å‰å¾€è´¦æˆ·è®¾ç½®æ›´æ”¹é‚®ç®±åœ°å€ã€‚"},  # æ­£ç¡®åŒ¹é…
    {"question": "è´µå…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "é”™è¯¯ç­”æ¡ˆã€‚"},  # æ•…æ„é”™è¯¯åŒ¹é…
    {"question": "å¦‚ä½•è”ç³»å®¢æˆ·æ”¯æŒï¼Ÿ", "answer": "æ‚¨å¯ä»¥é€šè¿‡èŠå¤©æˆ–é‚®ä»¶è”ç³»å®¢æˆ·æ”¯æŒã€‚"},  # æ­£ç¡®åŒ¹é…
    {"question": "", "answer": "ç©ºè¾“å…¥æµ‹è¯•ã€‚"},  # ç©ºè¾“å…¥
    {"question": "é—®é¢˜è¶…é•¿æµ‹è¯•" * 1000, "answer": "è¶…é•¿é—®é¢˜æµ‹è¯•ã€‚"}  # è¶…é•¿è¾“å…¥
]

# å‘é€è¯·æ±‚å¹¶æ‰“å°ç»“æœ
for i, data in enumerate(test_data):
    print(f"æµ‹è¯•ç”¨ä¾‹ {i + 1}:")
    try:
        response = requests.post(url, json=data)
        print("è¯·æ±‚æ•°æ®:", data)
        print("å“åº”ç»“æœ:", response.json())
    except Exception as e:
        print("è¯·æ±‚å¤±è´¥:", str(e))
    print("\n")
æµ‹è¯•ç”¨ä¾‹ 1ï¼šæ­£ç¡®åŒ¹é…
æµ‹è¯•ç”¨ä¾‹ 1:
è¯·æ±‚æ•°æ®: {'question': 'å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ', 'answer': 'æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚'}
å“åº”ç»“æœ: {
    "prediction": 1,
    "probabilities": [[0.05, 0.95]],
    "response_time": "0.1234s"
}
æµ‹è¯•ç”¨ä¾‹ 2ï¼šæ­£ç¡®åŒ¹é…
æµ‹è¯•ç”¨ä¾‹ 2:
è¯·æ±‚æ•°æ®: {'question': 'å¦‚ä½•æ›´æ”¹è´¦æˆ·é‚®ç®±ï¼Ÿ', 'answer': 'è¯·å‰å¾€è´¦æˆ·è®¾ç½®æ›´æ”¹é‚®ç®±åœ°å€ã€‚'}
å“åº”ç»“æœ: {
    "prediction": 1,
    "probabilities": [[0.03, 0.97]],
    "response_time": "0.1345s"
}
æµ‹è¯•ç”¨ä¾‹ 3ï¼šæ•…æ„é”™è¯¯åŒ¹é…
æµ‹è¯•ç”¨ä¾‹ 3:
è¯·æ±‚æ•°æ®: {'question': 'è´µå…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ', 'answer': 'é”™è¯¯ç­”æ¡ˆã€‚'}
å“åº”ç»“æœ: {
    "prediction": 0,
    "probabilities": [[0.85, 0.15]],
    "response_time": "0.1456s"
}
æµ‹è¯•ç”¨ä¾‹ 4ï¼šæ­£ç¡®åŒ¹é…
æµ‹è¯•ç”¨ä¾‹ 4:
è¯·æ±‚æ•°æ®: {'question': 'å¦‚ä½•è”ç³»å®¢æˆ·æ”¯æŒï¼Ÿ', 'answer': 'æ‚¨å¯ä»¥é€šè¿‡èŠå¤©æˆ–é‚®ä»¶è”ç³»å®¢æˆ·æ”¯æŒã€‚'}
å“åº”ç»“æœ: {
    "prediction": 1,
    "probabilities": [[0.02, 0.98]],
    "response_time": "0.1123s"
}
æµ‹è¯•ç”¨ä¾‹ 5ï¼šç©ºè¾“å…¥æµ‹è¯•
æµ‹è¯•ç”¨ä¾‹ 5:
è¯·æ±‚æ•°æ®: {'question': '', 'answer': 'ç©ºè¾“å…¥æµ‹è¯•ã€‚'}
å“åº”ç»“æœ: {
    "error": "An error occurred while processing your request."
}
æµ‹è¯•ç”¨ä¾‹ 6ï¼šè¶…é•¿è¾“å…¥æµ‹è¯•
æµ‹è¯•ç”¨ä¾‹ 6:
è¯·æ±‚æ•°æ®: {'question': 'é—®é¢˜è¶…é•¿æµ‹è¯•é—®é¢˜è¶…é•¿æµ‹è¯•é—®é¢˜è¶…é•¿æµ‹è¯•...(çœç•¥)...', 'answer': 'è¶…é•¿é—®é¢˜æµ‹è¯•ã€‚'}
å“åº”ç»“æœ: {
    "error": "An error occurred while processing your request."
}
æ¨¡æ‹Ÿæ—¥å¿—æ–‡ä»¶ï¼ˆlogs/system.logï¼‰ï¼š
2024-11-17 12:00:15.123 | INFO     | Client IP: 127.0.0.1, Question: å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ, Answer: æ‚¨å¯ä»¥åœ¨è®¾ç½®é¡µé¢é‡ç½®å¯†ç ã€‚, Prediction: 1, Response Time: 0.1234s
2024-11-17 12:00:20.456 | INFO     | Client IP: 127.0.0.1, Question: å¦‚ä½•æ›´æ”¹è´¦æˆ·é‚®ç®±ï¼Ÿ, Answer: è¯·å‰å¾€è´¦æˆ·è®¾ç½®æ›´æ”¹é‚®ç®±åœ°å€ã€‚, Prediction: 1, Response Time: 0.1345s
2024-11-17 12:00:25.789 | INFO     | Client IP: 127.0.0.1, Question: è´µå…¬å¸çš„é€€æ¬¾æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ, Answer: é”™è¯¯ç­”æ¡ˆã€‚, Prediction: 0, Response Time: 0.1456s
2024-11-17 12:00:30.012 | INFO     | Client IP: 127.0.0.1, Question: å¦‚ä½•è”ç³»å®¢æˆ·æ”¯æŒï¼Ÿ, Answer: æ‚¨å¯ä»¥é€šè¿‡èŠå¤©æˆ–é‚®ä»¶è”ç³»å®¢æˆ·æ”¯æŒã€‚, Prediction: 1, Response Time: 0.1123s
2024-11-17 12:00:35.567 | ERROR    | Error processing request from 127.0.0.1: tokenizer input length exceeded maximum
PrometheusæŒ‡æ ‡é‡‡é›†ï¼š
ï¼ˆ1ï¼‰è¯·æ±‚è®¡æ•°ï¼ˆrequest_countï¼‰ï¼š
request_count: 6
ï¼ˆ2ï¼‰å“åº”æ—¶é—´ï¼ˆresponse_timeï¼‰ï¼š
response_time:
  Bucket (0.1s): 2
  Bucket (0.2s): 2
  Bucket (0.3s): 2
ï¼ˆ3ï¼‰å¼‚å¸¸è¯·æ±‚è®¡æ•°ï¼š
request_errors_total: 2




















