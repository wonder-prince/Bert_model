{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d751e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 使用SQuAD数据集来微调\n",
    "datasets = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783f4d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,BertForQuestionAnswering\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\",use_fast=True)\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0da09",
   "metadata": {},
   "source": [
    "## 下面开始对数据开始预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6f4ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962a60f5dade4967b5ccf0b3dff74a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data(examples):\n",
    "# 1. 正常分词，使用滑动窗口\n",
    "    inputs = tokenizer(\n",
    "        examples['question'], \n",
    "        examples['context'], \n",
    "        truncation=\"only_second\", \n",
    "        padding='max_length', \n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_mapping[i]\n",
    "        input_ids = inputs['input_ids'][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # 找到序列中 Context 的区间（区分 Question 和 Context）\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # 答案信息\n",
    "        answer = examples['answers'][sample_index]\n",
    "        \n",
    "        # 如果没有答案，直接指向 [CLS]\n",
    "        if len(answer['answer_start']) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answer['answer_start'][0]\n",
    "            end_char = start_char + len(answer['text'][0])\n",
    "            \n",
    "            # 找到当前分片在原始 Context 中的起始和结束 Token 索引\n",
    "            token_start_index = 0\n",
    "            token_end_index = 0\n",
    "            \n",
    "            # 找到 Context 在 input_ids 中的起始和结束位置\n",
    "            token_search_start = 0\n",
    "            while sequence_ids[token_search_start] != 1:\n",
    "                token_search_start += 1\n",
    "                \n",
    "            token_search_end = len(input_ids) - 1\n",
    "            while sequence_ids[token_search_end] != 1:\n",
    "                token_search_end -= 1\n",
    "            \n",
    "            # 判断答案是否完全在当前分片的 Context 范围内\n",
    "            if not (offsets[token_search_start][0] <= start_char and offsets[token_search_end][1] >= end_char):\n",
    "                # 答案不在这个分片里，设为 [CLS]\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                # 答案在分片里，开始精确定位\n",
    "                curr = token_search_start\n",
    "                while curr <= token_search_end and offsets[curr][0] <= start_char:\n",
    "                    curr += 1\n",
    "                start_positions.append(curr - 1)\n",
    "                \n",
    "                curr = token_search_end\n",
    "                while curr >= token_search_start and offsets[curr][1] >= end_char:\n",
    "                    curr -= 1\n",
    "                end_positions.append(curr + 1)\n",
    "\n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['end_positions'] = end_positions \n",
    "    return inputs\n",
    "\n",
    "# 应用预处理函数\n",
    "tokenized_dataset = datasets.map(\n",
    "    preprocess_data, \n",
    "    batched=True, \n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1001c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据集进行划分\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56024687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2486' max='32940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2486/32940 15:11 < 3:06:14, 2.73 it/s, Epoch 0.15/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     12\u001b[39m trainer = Trainer(\n\u001b[32m     13\u001b[39m     model=model,\n\u001b[32m     14\u001b[39m     args=training_args,\n\u001b[32m     15\u001b[39m     train_dataset=train_dataset,\n\u001b[32m     16\u001b[39m     eval_dataset=eval_dataset,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_data\\envs\\bert\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_data\\envs\\bert\\Lib\\site-packages\\transformers\\trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-qa\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,         # 学习率\n",
    "    per_device_train_batch_size=8, # 训练批次大小\n",
    "    per_device_eval_batch_size=8,  # 评估批次大小\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,  \n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
