{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d751e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 使用SQuAD数据集来微调\n",
    "datasets = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783f4d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc2db00c257426096b602c57cf635a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e4cbd04a714a089a0337daaefafe66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772862c6b1e44833902a8a9c61f7cd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bd89320eaa4fd08c0973432b8621d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5570692e964eb0b54d3b9982434397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,BertForQuestionAnswering\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\",use_fast=True)\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0da09",
   "metadata": {},
   "source": [
    "- 下面开始对数据开始预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6f4ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3ac533ec3e486e9479677a639c36d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf01dbf737d4fbda035920efdeaebf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data(examples):\n",
    "# 1. 正常分词，使用滑动窗口\n",
    "    inputs = tokenizer(\n",
    "        examples['question'], \n",
    "        examples['context'], \n",
    "        truncation=\"only_second\", \n",
    "        padding='max_length', \n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_mapping[i]\n",
    "        input_ids = inputs['input_ids'][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # 找到序列中 Context 的区间（区分 Question 和 Context）\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # 答案信息\n",
    "        answer = examples['answers'][sample_index]\n",
    "        \n",
    "        # 如果没有答案，直接指向 [CLS]\n",
    "        if len(answer['answer_start']) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answer['answer_start'][0]\n",
    "            end_char = start_char + len(answer['text'][0])\n",
    "            \n",
    "            # 找到当前分片在原始 Context 中的起始和结束 Token 索引\n",
    "            token_start_index = 0\n",
    "            token_end_index = 0\n",
    "            \n",
    "            # 找到 Context 在 input_ids 中的起始和结束位置\n",
    "            token_search_start = 0\n",
    "            while sequence_ids[token_search_start] != 1:\n",
    "                token_search_start += 1\n",
    "                \n",
    "            token_search_end = len(input_ids) - 1\n",
    "            while sequence_ids[token_search_end] != 1:\n",
    "                token_search_end -= 1\n",
    "            \n",
    "            # 判断答案是否完全在当前分片的 Context 范围内\n",
    "            if not (offsets[token_search_start][0] <= start_char and offsets[token_search_end][1] >= end_char):\n",
    "                # 答案不在这个分片里，设为 [CLS]\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                # 答案在分片里，开始精确定位\n",
    "                curr = token_search_start\n",
    "                while curr <= token_search_end and offsets[curr][0] <= start_char:\n",
    "                    curr += 1\n",
    "                start_positions.append(curr - 1)\n",
    "                \n",
    "                curr = token_search_end\n",
    "                while curr >= token_search_start and offsets[curr][1] >= end_char:\n",
    "                    curr -= 1\n",
    "                end_positions.append(curr + 1)\n",
    "\n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['end_positions'] = end_positions \n",
    "    return inputs\n",
    "\n",
    "# 应用预处理函数\n",
    "tokenized_dataset = datasets.map(\n",
    "    preprocess_data, \n",
    "    batched=True, \n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1001c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据集进行划分\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e52575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 版本: 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]\n",
      "PyTorch 版本: 2.6.0+cu124\n",
      "CUDA 是否可用: False\n",
      "无法加载 NVIDIA 驱动库: module 'ctypes' has no attribute 'WinDLL'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 是否可用: {torch.cuda.is_available()}\")\n",
    "if not torch.cuda.is_available():\n",
    "    import ctypes\n",
    "    # 尝试加载 nvcuda.dll (Windows) 看看驱动库是否存在\n",
    "    try:\n",
    "        ctypes.WinDLL('nvcuda.dll')\n",
    "        print(\"NVIDIA 驱动库 (nvcuda.dll) 检测正常\")\n",
    "    except Exception as e:\n",
    "        print(f\"无法加载 NVIDIA 驱动库: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56024687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32940' max='32940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32940/32940 13:18:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.018800</td>\n",
       "      <td>1.052049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.657200</td>\n",
       "      <td>1.194216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32940, training_loss=0.951408569669463, metrics={'train_runtime': 47886.8206, 'train_samples_per_second': 5.503, 'train_steps_per_second': 0.688, 'total_flos': 5.164033933049242e+16, 'train_loss': 0.951408569669463, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-qa\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,         # 学习率\n",
    "    per_device_train_batch_size=8, # 训练批次大小\n",
    "    per_device_eval_batch_size=8,  # 评估批次大小\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,  \n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
