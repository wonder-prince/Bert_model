{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1eda64c",
   "metadata": {},
   "source": [
    "# 基于SQuAD的数据集微调 Bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d751e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 使用SQuAD数据集来微调\n",
    "datasets = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "783f4d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,BertForQuestionAnswering\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\",use_fast=True)\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0da09",
   "metadata": {},
   "source": [
    "- 下面开始对数据开始预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6f4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "# 1. 正常分词，使用滑动窗口\n",
    "    inputs = tokenizer(\n",
    "        examples['question'], \n",
    "        examples['context'], \n",
    "        truncation=\"only_second\", \n",
    "        padding='max_length', \n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_mapping[i]\n",
    "        input_ids = inputs['input_ids'][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # 找到序列中 Context 的区间（区分 Question 和 Context）\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # 答案信息\n",
    "        answer = examples['answers'][sample_index]\n",
    "        \n",
    "        # 如果没有答案，直接指向 [CLS]\n",
    "        if len(answer['answer_start']) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answer['answer_start'][0]\n",
    "            end_char = start_char + len(answer['text'][0])\n",
    "            \n",
    "            # 找到当前分片在原始 Context 中的起始和结束 Token 索引\n",
    "            token_start_index = 0\n",
    "            token_end_index = 0\n",
    "            \n",
    "            # 找到 Context 在 input_ids 中的起始和结束位置\n",
    "            token_search_start = 0\n",
    "            while sequence_ids[token_search_start] != 1:\n",
    "                token_search_start += 1\n",
    "                \n",
    "            token_search_end = len(input_ids) - 1\n",
    "            while sequence_ids[token_search_end] != 1:\n",
    "                token_search_end -= 1\n",
    "            \n",
    "            # 判断答案是否完全在当前分片的 Context 范围内\n",
    "            if not (offsets[token_search_start][0] <= start_char and offsets[token_search_end][1] >= end_char):\n",
    "                # 答案不在这个分片里，设为 [CLS]\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                # 答案在分片里，开始精确定位\n",
    "                curr = token_search_start\n",
    "                while curr <= token_search_end and offsets[curr][0] <= start_char:\n",
    "                    curr += 1\n",
    "                start_positions.append(curr - 1)\n",
    "                \n",
    "                curr = token_search_end\n",
    "                while curr >= token_search_start and offsets[curr][1] >= end_char:\n",
    "                    curr -= 1\n",
    "                end_positions.append(curr + 1)\n",
    "\n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['end_positions'] = end_positions \n",
    "    return inputs\n",
    "\n",
    "# 应用预处理函数\n",
    "tokenized_dataset = datasets.map(\n",
    "    preprocess_data, \n",
    "    batched=True, \n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1001c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据集进行划分\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4e52575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 版本: 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:05:38) [MSC v.1929 64 bit (AMD64)]\n",
      "PyTorch 版本: 2.6.0+cu124\n",
      "CUDA 是否可用: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python 版本: {sys.version}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"CUDA 是否可用: {torch.cuda.is_available()}\")\n",
    "if not torch.cuda.is_available():\n",
    "    import ctypes\n",
    "    # 尝试加载 nvcuda.dll (Windows) 看看驱动库是否存在\n",
    "    try:\n",
    "        ctypes.WinDLL('nvcuda.dll')\n",
    "        print(\"NVIDIA 驱动库 (nvcuda.dll) 检测正常\")\n",
    "    except Exception as e:\n",
    "        print(f\"无法加载 NVIDIA 驱动库: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56024687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='213' max='32940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  213/32940 01:06 < 2:50:53, 3.19 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     12\u001b[39m trainer = Trainer(\n\u001b[32m     13\u001b[39m     model=model,\n\u001b[32m     14\u001b[39m     args=training_args,\n\u001b[32m     15\u001b[39m     train_dataset=train_dataset,\n\u001b[32m     16\u001b[39m     eval_dataset=eval_dataset,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_data\\envs\\bert\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_data\\envs\\bert\\Lib\\site-packages\\transformers\\trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-qa\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,         # 学习率\n",
    "    per_device_train_batch_size=8, # 训练批次大小\n",
    "    per_device_eval_batch_size=8,  # 评估批次大小\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,  \n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998251ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"What is the name of the repository for pre-trained transformers? \"\n",
    "context = \"Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. The library is maintained by the Hugging Face team and the open-source community. The models can be easily integrated into applications using the Transformers API and are available through the Hugging Face Model Hub.\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_index=torch.argmax(outputs.start_logits)\n",
    "end_index=torch.argmax(outputs.end_logits)+1\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_index:end_index]))\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee96f3e",
   "metadata": {},
   "source": [
    "由于huggingface并不会添加vocab_file在最后的权重文件当中，因此需要重新添加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecfcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修复前目录内容：\n",
      "['config.json', 'model.safetensors', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'trainer_state.json', 'training_args.bin']\n",
      "\n",
      "正在从 bert-base-uncased 复制 tokenizer 文件（vocab.txt 等）...\n",
      "\n",
      "修复完成！现在目录内容：\n",
      "['config.json', 'model.safetensors', 'optimizer.pt', 'rng_state.pth', 'scheduler.pt', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'trainer_state.json', 'training_args.bin', 'vocab.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 你的模型保存根目录\n",
    "model_path = \"../bert-qa/checkpoint-32940\"   # 确认是这个路径\n",
    "\n",
    "# 第一步：查看当前目录有哪些文件\n",
    "print(\"修复前目录内容：\")\n",
    "print(os.listdir(model_path))\n",
    "\n",
    "# 第二步：从 bert-base-uncased 下载并复制 tokenizer 文件到你的目录\n",
    "print(\"\\n正在从 bert-base-uncased 复制 tokenizer 文件（vocab.txt 等）...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# 第三步：查看修复后文件\n",
    "print(\"\\n修复完成！现在目录内容：\")\n",
    "print(os.listdir(model_path))\n",
    "\n",
    "# 你应该会看到多了这些文件：\n",
    "# vocab.txt\n",
    "# tokenizer_config.json\n",
    "# special_tokens_map.json\n",
    "# tokenizer.json（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffe62cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 测试结果 ===\n",
      "问题： What is the capital of France?\n",
      "答案： Paris\n",
      "置信度： 0.9952\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 直接使用根目录！（最保险的方式）\n",
    "model_path = \"../bert-qa/checkpoint-32940\"   \n",
    "\n",
    "# 加载 tokenizer 和 model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# 创建问答 pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1  # 有GPU用GPU，没有用CPU\n",
    ")\n",
    "\n",
    "# 测试你的 fine-tune 模型\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France is a country in Europe. The capital of France is Paris, which is known for its art, fashion, and culture.\"\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(\"\\n=== 测试结果 ===\")\n",
    "print(\"问题：\", question)\n",
    "print(\"答案：\", result['answer'])\n",
    "print(\"置信度：\", f\"{result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcb638",
   "metadata": {},
   "source": [
    "## 总结\n",
    "上面是使用数据集对bert进行了微调，其中涉及到了NLP中的分词、模型加载、tokenizer的创建、测试、评估的过程，是一个最小的微调示例，后续会在其中加入其他优化过程。\n",
    "其中，bert模型是一个英文语言训练模型，对英文问题的理解能力更好，中文则不行，可以尝试去改q&c，就会发现置信度的变化。\n",
    "\n",
    "[详细可以看bert_url](https://huggingface.co/google-bert/bert-base-uncased)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
