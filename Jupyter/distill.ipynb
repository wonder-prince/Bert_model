{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf524eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239940322a0e429aa8e2aa10f03ea47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cb11e468184607a6c2d97360da07a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fa645018f441ab9737b30d50e6f31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f4a3cbbbde4b23850bc9ab911e44b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e11b581f1949d59dd698ee3d9e6698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4caedd8715d4a3b92db7bec5778da77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b979cd016d6445c83fca4731067af8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c26379efa44f68a4ebf53b2796ce4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116e4e6546dc43979836936b1b232e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e4f14a18a84e56b17ab59e304f568c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distillation Loss: 0.06905972212553024\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, DistilBertModel, DistilBertForQuestionAnswering\n",
    "from transformers import DistilBertTokenizer, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# 加载BERT教师模型与DistilBERT学生模型\n",
    "teacher_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "student_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 使用相同的Tokenizer进行词汇预处理\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "distil_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 定义输入文本并进行编码\n",
    "text = \"Machine reading comprehension is essential for question-answering.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "distil_inputs = distil_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 获取教师模型输出\n",
    "with torch.no_grad():\n",
    "    teacher_outputs = teacher_model(**inputs).last_hidden_state\n",
    "\n",
    "# 学生模型的前向传播\n",
    "student_outputs = student_model(**distil_inputs).last_hidden_state\n",
    "\n",
    "# 定义蒸馏损失函数：使用均方误差（MSE）对齐学生与教师模型的输出\n",
    "distillation_loss = nn.MSELoss()(student_outputs, teacher_outputs)\n",
    "\n",
    "# 打印蒸馏损失\n",
    "print(\"Distillation Loss:\", distillation_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3dc28d",
   "metadata": {},
   "source": [
    "KL散度评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9da22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Distillation Loss: 0.3852507472038269\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def kl_distillation_loss(student_logits, teacher_logits, T=2.0):\n",
    "\n",
    "    # 计算软概率分布\n",
    "    # 注意：F.kl_div 的输入期望是 log_softmax，目标是 softmax\n",
    "    p_teacher = F.softmax(teacher_logits / T, dim=-1)\n",
    "    p_student = F.log_softmax(student_logits / T, dim=-1)\n",
    "    \n",
    "    # 计算 KL 散度\n",
    "    # reduction='batchmean' 是数学上标准的 KL 散度计算方式\n",
    "    loss = F.kl_div(p_student, p_teacher, reduction='batchmean') * (T ** 2)\n",
    "    return loss\n",
    "\n",
    "# 4. 计算并打印损失\n",
    "loss = kl_distillation_loss(student_outputs, teacher_outputs, T=2.0)\n",
    "print(\"KL Distillation Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5c929",
   "metadata": {},
   "source": [
    "循环蒸馏效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c271d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Distillation Loss: 0.0708\n",
      "Epoch 2\n",
      "Average Distillation Loss: 0.0597\n",
      "Epoch 3\n",
      "Average Distillation Loss: 0.0515\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# 这里也可以用其他优化器\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
    "texts = [\"Machine learning is the study of algorithms.\",\n",
    "         \"Natural Language Processing involves understanding human languages.\"]\n",
    "labels = [\"It is a subset of AI.\", \"A field in AI focusing on language.\"]\n",
    "\n",
    "# 蒸馏训练循环\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "    for text, label in zip(texts, labels):\n",
    "        # 准备输入\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        distil_inputs = distil_tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # 获取教师模型输出\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(**inputs).last_hidden_state\n",
    "        \n",
    "        # 获取学生模型输出\n",
    "        student_outputs = student_model(**distil_inputs).last_hidden_state\n",
    "        \n",
    "        # 计算蒸馏损失\n",
    "        loss = nn.MSELoss()(student_outputs, teacher_outputs)\n",
    "        \n",
    "        # 反向传播与优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 记录损失\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(texts)\n",
    "    print(f\"Average Distillation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180091e7",
   "metadata": {},
   "source": [
    "上面是直接进行加载dstill模型，下面要自己尝试进行distill操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4606658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.7049428224563599\n",
      "Epoch 2 - Loss: 0.6767551898956299\n",
      "Epoch 3 - Loss: 0.5498147010803223\n"
     ]
    }
   ],
   "source": [
    "# 冻结和解冻BERT模型\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 加载预训练的BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 小数据进行单层微调\n",
    "sentences = [\"The book is great!\", \"The movie was terrible.\"]\n",
    "labels = [1, 0]  # 假设1代表积极，0代表消极\n",
    "\n",
    "# 数据预处理\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 冻结所有BERT的层\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 解冻特定的层\n",
    "for param in model.bert.encoder.layer[-2:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义优化器，仅优化解冻层的参数\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "# 训练过程示例\n",
    "model.train()\n",
    "for epoch in range(3):  # 训练3个周期\n",
    "    outputs = model(**inputs, labels=torch.tensor(labels))\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
