{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf524eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "\n",
    "# hugging-face 国内镜像源\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c26746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, DistilBertModel, DistilBertForQuestionAnswering\n",
    "from transformers import DistilBertTokenizer, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# 加载BERT教师模型与DistilBERT学生模型\n",
    "teacher_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "student_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 使用相同的Tokenizer进行词汇预处理\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "distil_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 定义输入文本并进行编码\n",
    "text = \"Machine reading comprehension is essential for question-answering.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "distil_inputs = distil_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 获取教师模型输出\n",
    "with torch.no_grad():\n",
    "    teacher_outputs = teacher_model(**inputs).last_hidden_state\n",
    "\n",
    "# 学生模型的前向传播\n",
    "student_outputs = student_model(**distil_inputs).last_hidden_state\n",
    "\n",
    "# 定义蒸馏损失函数：使用均方误差（MSE）对齐学生与教师模型的输出\n",
    "distillation_loss = nn.MSELoss()(student_outputs, teacher_outputs)\n",
    "\n",
    "# 打印蒸馏损失\n",
    "print(\"Distillation Loss:\", distillation_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3dc28d",
   "metadata": {},
   "source": [
    "KL散度评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9da22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Distillation Loss: 0.3852507472038269\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def kl_distillation_loss(student_logits, teacher_logits, T=2.0):\n",
    "\n",
    "    # 计算软概率分布\n",
    "    # 注意：F.kl_div 的输入期望是 log_softmax，目标是 softmax\n",
    "    p_teacher = F.softmax(teacher_logits / T, dim=-1)\n",
    "    p_student = F.log_softmax(student_logits / T, dim=-1)\n",
    "    \n",
    "    # 计算 KL 散度\n",
    "    # reduction='batchmean' 是数学上标准的 KL 散度计算方式\n",
    "    loss = F.kl_div(p_student, p_teacher, reduction='batchmean') * (T ** 2)\n",
    "    return loss\n",
    "\n",
    "# 4. 计算并打印损失\n",
    "loss = kl_distillation_loss(student_outputs, teacher_outputs, T=2.0)\n",
    "print(\"KL Distillation Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5c929",
   "metadata": {},
   "source": [
    "循环蒸馏效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c271d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Distillation Loss: 0.0708\n",
      "Epoch 2\n",
      "Average Distillation Loss: 0.0597\n",
      "Epoch 3\n",
      "Average Distillation Loss: 0.0515\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# 这里也可以用其他优化器\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
    "texts = [\"Machine learning is the study of algorithms.\",\n",
    "         \"Natural Language Processing involves understanding human languages.\"]\n",
    "labels = [\"It is a subset of AI.\", \"A field in AI focusing on language.\"]\n",
    "\n",
    "# 蒸馏训练循环\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "    for text, label in zip(texts, labels):\n",
    "        # 准备输入\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        distil_inputs = distil_tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # 获取教师模型输出\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(**inputs).last_hidden_state\n",
    "        \n",
    "        # 获取学生模型输出\n",
    "        student_outputs = student_model(**distil_inputs).last_hidden_state\n",
    "        \n",
    "        # 计算蒸馏损失\n",
    "        loss = nn.MSELoss()(student_outputs, teacher_outputs)\n",
    "        \n",
    "        # 反向传播与优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 记录损失\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(texts)\n",
    "    print(f\"Average Distillation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180091e7",
   "metadata": {},
   "source": [
    "上面是直接进行加载dstill模型，下面要自己尝试进行distill操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4606658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.7049428224563599\n",
      "Epoch 2 - Loss: 0.6767551898956299\n",
      "Epoch 3 - Loss: 0.5498147010803223\n"
     ]
    }
   ],
   "source": [
    "# 冻结和解冻BERT模型\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 加载预训练的BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 小数据进行单层微调\n",
    "sentences = [\"The book is great!\", \"The movie was terrible.\"]\n",
    "labels = [1, 0]  # 假设1代表积极，0代表消极\n",
    "\n",
    "# 数据预处理\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 冻结所有BERT的层\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 解冻特定的层\n",
    "for param in model.bert.encoder.layer[-2:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义优化器，仅优化解冻层的参数\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "# 训练过程示例\n",
    "model.train()\n",
    "for epoch in range(3):  # 训练3个周期\n",
    "    outputs = model(**inputs, labels=torch.tensor(labels))\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc497b7c",
   "metadata": {},
   "source": [
    "下面对学习率设置进行学习：\n",
    "学习率决定了梯度更新的步长，如何避免模型错过最优解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8400eaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Average training loss: 0.6955\n",
      "\n",
      "Epoch 2/3\n",
      "Average training loss: 0.5635\n",
      "\n",
      "Epoch 3/3\n",
      "Average training loss: 0.4875\n",
      "\n",
      "部分模型参数示例：\n",
      "classifier.weight: tensor([[-0.0259, -0.0196, -0.0052,  ...,  0.0098,  0.0145, -0.0071],\n",
      "        [ 0.0108,  0.0146,  0.0211,  ...,  0.0420, -0.0225,  0.0074]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 加载数据集（示例数据）\n",
    "data = [\n",
    "    (\"The company posted a significant increase in quarterly revenue.\", 0),\n",
    "    (\"New heart disease medication approved by FDA.\", 1),\n",
    "    (\"Stock market affected by global events.\", 0),\n",
    "    (\"Medical advancements in treating rare diseases.\", 1)\n",
    "]\n",
    "labels = [item[1] for item in data]\n",
    "texts = [item[0] for item in data]\n",
    "\n",
    "# 实例化Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 将标签转换为张量\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# 加载预训练的BERT模型并调整参数\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 将模型设置为训练模式\n",
    "model.train()\n",
    "\n",
    "# 定义优化器和学习率调度器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 3\n",
    "total_steps = len(inputs[\"input_ids\"]) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# 将数据加载至DataLoader\n",
    "train_data = DataLoader(list(zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], labels_tensor)), batch_size=2)\n",
    "\n",
    "# 微调BERT模型\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    for batch in train_data:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播，获取损失\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 梯度裁剪，避免梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_data)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 测试阶段：打印模型参数信息\n",
    "print(\"\\n部分模型参数示例：\")\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        print(f\"{name}: {param[:2]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688e417",
   "metadata": {},
   "source": [
    "更加高级的微调方式，是对参数进行高效微调\n",
    "[LoRA论文连接](https://arxiv.org/pdf/2106.09685)\n",
    "[Pre-Tuning论文连接](https://arxiv.org/abs/2101.00190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97492d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-mirror.com', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='hf-mirror.com', port=443) at 0x768082c95640>, 'Connection to hf-mirror.com timed out. (connect timeout=10)'))\"), '(Request ID: c9cc11ad-a5f9-403f-917e-fbcee580f4c2)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-mirror.com', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='hf-mirror.com', port=443) at 0x768082c94b90>, 'Connection to hf-mirror.com timed out. (connect timeout=10)'))\"), '(Request ID: 7599ef6d-d70d-40b5-a8b9-c8933071e81e)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-mirror.com\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NewConnectionError(\"HTTPSConnection(host=\\'hf-mirror.com\\', port=443): Failed to establish a new connection: [Errno 111] Connection refused\"))'), '(Request ID: 72a394d3-0153-4860-b8f6-0e40e37268ac)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-mirror.com\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NewConnectionError(\"HTTPSConnection(host=\\'hf-mirror.com\\', port=443): Failed to establish a new connection: [Errno 111] Connection refused\"))'), '(Request ID: 2b8316b4-543d-4fc8-b5cc-02ffd935d64b)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-mirror.com\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NewConnectionError(\"HTTPSConnection(host=\\'hf-mirror.com\\', port=443): Failed to establish a new connection: [Errno 111] Connection refused\"))'), '(Request ID: 0c93ac4c-3702-40f7-bd52-5d68eb4d59e2)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3534093499183655\n",
      "Epoch 2, Loss: 0.2340371161699295\n",
      "Epoch 3, Loss: 0.30949392914772034\n",
      "Output Embeddings: tensor([[[-0.5007,  0.0067,  0.4987,  ..., -0.6998, -0.3988,  0.8621],\n",
      "         [-0.3269,  0.9228, -0.1940,  ..., -0.4193,  0.0240,  0.1988],\n",
      "         [-0.3774,  0.9690, -0.0234,  ..., -0.3832, -0.2031, -0.0308],\n",
      "         ...,\n",
      "         [-0.8675,  0.1132, -0.0237,  ..., -0.6927,  0.0622,  0.7820],\n",
      "         [-0.7119,  0.6516,  0.4629,  ..., -0.1070,  0.0572,  0.9439],\n",
      "         [-0.7524,  0.4928,  0.3584,  ..., -1.4589,  0.9578,  0.5290]]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化BERT模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 定义LoRA插入函数\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, input_dim, rank):\n",
    "        super(LoRA, self).__init__()\n",
    "        # 定义低秩矩阵\n",
    "        self.low_rank_left = nn.Parameter(torch.randn(input_dim, rank))# A\n",
    "        self.low_rank_right = nn.Parameter(torch.randn(rank, input_dim))# B\n",
    "        self.scaling_factor = 1.0 / (rank ** 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 低秩矩阵的插入\n",
    "        lora_update = torch.matmul(self.low_rank_left, self.low_rank_right) * self.scaling_factor\n",
    "        return x + torch.matmul(x, lora_update)\n",
    "\n",
    "# 将LoRA应用到模型的encoder层\n",
    "for layer in model.encoder.layer:\n",
    "    layer.attention.self.query = LoRA(layer.attention.self.query.in_features, rank=8)\n",
    "\n",
    "# 定义Prefix Tuning类\n",
    "class PrefixTuning(nn.Module):\n",
    "    def __init__(self, model, prefix_length=10, hidden_size=768):\n",
    "        super(PrefixTuning, self).__init__()\n",
    "        # 创建前缀向量\n",
    "        self.prefix_embeddings = nn.Parameter(torch.randn(prefix_length, hidden_size))\n",
    "        self.prefix_length = prefix_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取输入嵌入\n",
    "        original_embeddings = self.model.embeddings(input_ids)\n",
    "        \n",
    "        # 将前缀添加到输入\n",
    "        batch_size = input_ids.size(0)\n",
    "        prefix_embeddings = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        modified_embeddings = torch.cat([prefix_embeddings, original_embeddings], dim=1)\n",
    "        \n",
    "        # 调整attention mask\n",
    "        extended_attention_mask = torch.cat([torch.ones(batch_size, self.prefix_length).to(attention_mask.device), attention_mask], dim=1)\n",
    "        return self.model(inputs_embeds=modified_embeddings, attention_mask=extended_attention_mask)\n",
    "\n",
    "# 将Prefix Tuning集成到BERT中\n",
    "prefix_tuning = PrefixTuning(model)\n",
    "optimizer = optim.Adam(prefix_tuning.parameters(), lr=1e-5)\n",
    "\n",
    "# 准备示例数据\n",
    "text = \"LoRA and Prefix Tuning are efficient methods for adapting large models.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# 模型训练流程\n",
    "prefix_tuning.train()\n",
    "for epoch in range(3):  # 训练3个epoch\n",
    "    optimizer.zero_grad()\n",
    "    outputs = prefix_tuning(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    loss = (last_hidden_states ** 2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# 测试流程\n",
    "prefix_tuning.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = prefix_tuning(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    print(\"Output Embeddings:\", outputs.last_hidden_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. 设置环境与设备\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. 定义 SVD-LoRA 层 (创新点：将权重分解为 P * Sigma * Q)\n",
    "class SVDLoRALinear(nn.Module):\n",
    "    def __init__(self, original_layer, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.original_layer.weight.requires_grad = False  # 冻结原参数\n",
    "        \n",
    "        out_features = original_layer.out_features\n",
    "        in_features = original_layer.in_features\n",
    "        \n",
    "        # SVD 分解结构：Delta W = U * diag(S) * V\n",
    "        self.U = nn.Parameter(torch.randn(out_features, rank) * 0.01)\n",
    "        self.S = nn.Parameter(torch.ones(rank))  # 奇异值对角线\n",
    "        self.V = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n",
    "        \n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 原始权重路径\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # SVD 旁路路径: x @ (V.T @ diag(S) @ U.T)\n",
    "        # 优化计算顺序: (x @ V.T) * S @ U.T\n",
    "        lora_output = (x @ self.V.t()) * self.S\n",
    "        lora_output = lora_output @ self.U.t()\n",
    "        \n",
    "        return original_output + lora_output * self.scaling\n",
    "\n",
    "# 3. 定义集成模型 (Prefix-Tuning + SVD-LoRA)\n",
    "class HybridSVDModel(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", prefix_len=10, lora_rank=8):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.prefix_len = prefix_len\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # 注入 SVD-LoRA 到所有 Query 和 Value 层\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            layer.attention.self.query = SVDLoRALinear(layer.attention.self.query, rank=lora_rank)\n",
    "            layer.attention.self.value = SVDLoRALinear(layer.attention.self.value, rank=lora_rank)\n",
    "            \n",
    "        # Prefix Embedding (连续向量)\n",
    "        self.prefix_embedding = nn.Parameter(torch.randn(prefix_len, hidden_size))\n",
    "        \n",
    "        # 任务头 (以分类任务为例)\n",
    "        self.classifier = nn.Linear(hidden_size, 2) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # 1. 处理 Prefix\n",
    "        raw_embeds = self.bert.embeddings(input_ids)\n",
    "        prefix_embeds = self.prefix_embedding.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        inputs_embeds = torch.cat([prefix_embeds, raw_embeds], dim=1)\n",
    "        \n",
    "        # 2. 扩展 Attention Mask\n",
    "        prefix_mask = torch.ones(batch_size, self.prefix_len).to(device)\n",
    "        full_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # 3. 经过 BERT (含 LoRA 旁路)\n",
    "        outputs = self.bert(inputs_embeds=inputs_embeds, attention_mask=full_mask)\n",
    "        \n",
    "        # 取 [CLS] token 的输出进行分类\n",
    "        # 注意：由于加了 prefix，[CLS] 的位置现在在 index = self.prefix_len\n",
    "        cls_output = outputs.last_hidden_state[:, self.prefix_len, :]\n",
    "        return self.classifier(cls_output)\n",
    "\n",
    "# 4. 训练准备\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = HybridSVDModel().to(device)\n",
    "\n",
    "# 只优化 Prefix, LoRA 矩阵 和 Classifier\n",
    "trainable_params = [\n",
    "    {'params': [p for n, p in model.named_parameters() if 'U' in n or 'V' in n or 'S' in n or 'prefix' in n or 'classifier' in n]}\n",
    "]\n",
    "optimizer = optim.AdamW(trainable_params, lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 示例数据\n",
    "texts = [\"The movie was fantastic!\", \"I hated this film.\"]\n",
    "labels = torch.tensor([1, 0]).to(device)\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# 5. 训练循环\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    loss = criterion(logits, labels)\n",
    "    \n",
    "    # 创新点：添加奇异值稀疏化惩罚 (类似于 AdaLoRA 的正则项)\n",
    "    # 强制模型只使用最重要的“秩”\n",
    "    l1_reg = 0.0\n",
    "    for n, p in model.named_parameters():\n",
    "        if '.S' in n:\n",
    "            l1_reg += torch.norm(p, 1)\n",
    "    \n",
    "    total_loss = loss + 0.01 * l1_reg\n",
    "    \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f} | Reg: {l1_reg.item():.4f}\")\n",
    "\n",
    "# 6. 观察奇异值 (模型解释性)\n",
    "print(\"\\nTop 5 Singular Values in Layer 0 Query:\")\n",
    "print(model.bert.encoder.layer[0].attention.self.query.S[:5].detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
