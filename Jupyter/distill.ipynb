{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf524eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7897\"\n",
    "\n",
    "# hugging-face 国内镜像源\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c26746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, DistilBertModel, DistilBertForQuestionAnswering\n",
    "from transformers import DistilBertTokenizer, BertTokenizer\n",
    "import torch.nn as nn\n",
    "\n",
    "# 加载BERT教师模型与DistilBERT学生模型\n",
    "teacher_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "student_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 使用相同的Tokenizer进行词汇预处理\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "distil_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 定义输入文本并进行编码\n",
    "text = \"Machine reading comprehension is essential for question-answering.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "distil_inputs = distil_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 获取教师模型输出\n",
    "with torch.no_grad():\n",
    "    teacher_outputs = teacher_model(**inputs).last_hidden_state\n",
    "\n",
    "# 学生模型的前向传播\n",
    "student_outputs = student_model(**distil_inputs).last_hidden_state\n",
    "\n",
    "# 定义蒸馏损失函数：使用均方误差（MSE）对齐学生与教师模型的输出\n",
    "distillation_loss = nn.MSELoss()(student_outputs, teacher_outputs)\n",
    "\n",
    "# 打印蒸馏损失\n",
    "print(\"Distillation Loss:\", distillation_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3dc28d",
   "metadata": {},
   "source": [
    "KL散度评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9da22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Distillation Loss: 0.3852507472038269\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def kl_distillation_loss(student_logits, teacher_logits, T=2.0):\n",
    "\n",
    "    # 计算软概率分布\n",
    "    # 注意：F.kl_div 的输入期望是 log_softmax，目标是 softmax\n",
    "    p_teacher = F.softmax(teacher_logits / T, dim=-1)\n",
    "    p_student = F.log_softmax(student_logits / T, dim=-1)\n",
    "    \n",
    "    # 计算 KL 散度\n",
    "    # reduction='batchmean' 是数学上标准的 KL 散度计算方式\n",
    "    loss = F.kl_div(p_student, p_teacher, reduction='batchmean') * (T ** 2)\n",
    "    return loss\n",
    "\n",
    "# 4. 计算并打印损失\n",
    "loss = kl_distillation_loss(student_outputs, teacher_outputs, T=2.0)\n",
    "print(\"KL Distillation Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5c929",
   "metadata": {},
   "source": [
    "循环蒸馏效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c271d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Average Distillation Loss: 0.0708\n",
      "Epoch 2\n",
      "Average Distillation Loss: 0.0597\n",
      "Epoch 3\n",
      "Average Distillation Loss: 0.0515\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "# 这里也可以用其他优化器\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
    "texts = [\"Machine learning is the study of algorithms.\",\n",
    "         \"Natural Language Processing involves understanding human languages.\"]\n",
    "labels = [\"It is a subset of AI.\", \"A field in AI focusing on language.\"]\n",
    "\n",
    "# 蒸馏训练循环\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    total_loss = 0\n",
    "    for text, label in zip(texts, labels):\n",
    "        # 准备输入\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        distil_inputs = distil_tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # 获取教师模型输出\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(**inputs).last_hidden_state\n",
    "        \n",
    "        # 获取学生模型输出\n",
    "        student_outputs = student_model(**distil_inputs).last_hidden_state\n",
    "        \n",
    "        # 计算蒸馏损失\n",
    "        loss = nn.MSELoss()(student_outputs, teacher_outputs)\n",
    "        \n",
    "        # 反向传播与优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 记录损失\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(texts)\n",
    "    print(f\"Average Distillation Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180091e7",
   "metadata": {},
   "source": [
    "上面是直接进行加载dstill模型，下面要自己尝试进行distill操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4606658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.7049428224563599\n",
      "Epoch 2 - Loss: 0.6767551898956299\n",
      "Epoch 3 - Loss: 0.5498147010803223\n"
     ]
    }
   ],
   "source": [
    "# 冻结和解冻BERT模型\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 加载预训练的BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 小数据进行单层微调\n",
    "sentences = [\"The book is great!\", \"The movie was terrible.\"]\n",
    "labels = [1, 0]  # 假设1代表积极，0代表消极\n",
    "\n",
    "# 数据预处理\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 冻结所有BERT的层\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 解冻特定的层\n",
    "for param in model.bert.encoder.layer[-2:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 定义优化器，仅优化解冻层的参数\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "# 训练过程示例\n",
    "model.train()\n",
    "for epoch in range(3):  # 训练3个周期\n",
    "    outputs = model(**inputs, labels=torch.tensor(labels))\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc497b7c",
   "metadata": {},
   "source": [
    "下面对学习率设置进行学习：\n",
    "学习率决定了梯度更新的步长，如何避免模型错过最优解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8400eaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Average training loss: 0.6955\n",
      "\n",
      "Epoch 2/3\n",
      "Average training loss: 0.5635\n",
      "\n",
      "Epoch 3/3\n",
      "Average training loss: 0.4875\n",
      "\n",
      "部分模型参数示例：\n",
      "classifier.weight: tensor([[-0.0259, -0.0196, -0.0052,  ...,  0.0098,  0.0145, -0.0071],\n",
      "        [ 0.0108,  0.0146,  0.0211,  ...,  0.0420, -0.0225,  0.0074]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 加载数据集（示例数据）\n",
    "data = [\n",
    "    (\"The company posted a significant increase in quarterly revenue.\", 0),\n",
    "    (\"New heart disease medication approved by FDA.\", 1),\n",
    "    (\"Stock market affected by global events.\", 0),\n",
    "    (\"Medical advancements in treating rare diseases.\", 1)\n",
    "]\n",
    "labels = [item[1] for item in data]\n",
    "texts = [item[0] for item in data]\n",
    "\n",
    "# 实例化Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 将标签转换为张量\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# 加载预训练的BERT模型并调整参数\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 将模型设置为训练模式\n",
    "model.train()\n",
    "\n",
    "# 定义优化器和学习率调度器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 3\n",
    "total_steps = len(inputs[\"input_ids\"]) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# 将数据加载至DataLoader\n",
    "train_data = DataLoader(list(zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], labels_tensor)), batch_size=2)\n",
    "\n",
    "# 微调BERT模型\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    for batch in train_data:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播，获取损失\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 梯度裁剪，避免梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_data)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 测试阶段：打印模型参数信息\n",
    "print(\"\\n部分模型参数示例：\")\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        print(f\"{name}: {param[:2]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688e417",
   "metadata": {},
   "source": [
    "更加高级的微调方式，是对参数进行高效微调\n",
    "[LoRA论文连接](https://arxiv.org/pdf/2106.09685)\n",
    "[Pre-Tuning论文连接](https://arxiv.org/abs/2101.00190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97492d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-mirror.com', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='hf-mirror.com', port=443) at 0x768082c95640>, 'Connection to hf-mirror.com timed out. (connect timeout=10)'))\"), '(Request ID: c9cc11ad-a5f9-403f-917e-fbcee580f4c2)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-mirror.com', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='hf-mirror.com', port=443) at 0x768082c94b90>, 'Connection to hf-mirror.com timed out. (connect timeout=10)'))\"), '(Request ID: 7599ef6d-d70d-40b5-a8b9-c8933071e81e)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-mirror.com\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NewConnectionError(\"HTTPSConnection(host=\\'hf-mirror.com\\', port=443): Failed to establish a new connection: [Errno 111] Connection refused\"))'), '(Request ID: 72a394d3-0153-4860-b8f6-0e40e37268ac)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-mirror.com\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NewConnectionError(\"HTTPSConnection(host=\\'hf-mirror.com\\', port=443): Failed to establish a new connection: [Errno 111] Connection refused\"))'), '(Request ID: 2b8316b4-543d-4fc8-b5cc-02ffd935d64b)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'hf-mirror.com\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NewConnectionError(\"HTTPSConnection(host=\\'hf-mirror.com\\', port=443): Failed to establish a new connection: [Errno 111] Connection refused\"))'), '(Request ID: 0c93ac4c-3702-40f7-bd52-5d68eb4d59e2)')' thrown while requesting HEAD https://hf-mirror.com/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3534093499183655\n",
      "Epoch 2, Loss: 0.2340371161699295\n",
      "Epoch 3, Loss: 0.30949392914772034\n",
      "Output Embeddings: tensor([[[-0.5007,  0.0067,  0.4987,  ..., -0.6998, -0.3988,  0.8621],\n",
      "         [-0.3269,  0.9228, -0.1940,  ..., -0.4193,  0.0240,  0.1988],\n",
      "         [-0.3774,  0.9690, -0.0234,  ..., -0.3832, -0.2031, -0.0308],\n",
      "         ...,\n",
      "         [-0.8675,  0.1132, -0.0237,  ..., -0.6927,  0.0622,  0.7820],\n",
      "         [-0.7119,  0.6516,  0.4629,  ..., -0.1070,  0.0572,  0.9439],\n",
      "         [-0.7524,  0.4928,  0.3584,  ..., -1.4589,  0.9578,  0.5290]]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化BERT模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 定义LoRA插入函数\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, input_dim, rank):\n",
    "        super(LoRA, self).__init__()\n",
    "        # 定义低秩矩阵\n",
    "        self.low_rank_left = nn.Parameter(torch.randn(input_dim, rank))# A\n",
    "        self.low_rank_right = nn.Parameter(torch.randn(rank, input_dim))# B\n",
    "        self.scaling_factor = 1.0 / (rank ** 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 低秩矩阵的插入\n",
    "        lora_update = torch.matmul(self.low_rank_left, self.low_rank_right) * self.scaling_factor\n",
    "        return x + torch.matmul(x, lora_update)\n",
    "\n",
    "# 将LoRA应用到模型的encoder层\n",
    "for layer in model.encoder.layer:\n",
    "    layer.attention.self.query = LoRA(layer.attention.self.query.in_features, rank=8)\n",
    "\n",
    "# 定义Prefix Tuning类\n",
    "class PrefixTuning(nn.Module):\n",
    "    def __init__(self, model, prefix_length=10, hidden_size=768):\n",
    "        super(PrefixTuning, self).__init__()\n",
    "        # 创建前缀向量\n",
    "        self.prefix_embeddings = nn.Parameter(torch.randn(prefix_length, hidden_size))\n",
    "        self.prefix_length = prefix_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取输入嵌入\n",
    "        original_embeddings = self.model.embeddings(input_ids)\n",
    "        \n",
    "        # 将前缀添加到输入\n",
    "        batch_size = input_ids.size(0)\n",
    "        prefix_embeddings = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        modified_embeddings = torch.cat([prefix_embeddings, original_embeddings], dim=1)\n",
    "        \n",
    "        # 调整attention mask\n",
    "        extended_attention_mask = torch.cat([torch.ones(batch_size, self.prefix_length).to(attention_mask.device), attention_mask], dim=1)\n",
    "        return self.model(inputs_embeds=modified_embeddings, attention_mask=extended_attention_mask)\n",
    "\n",
    "# 将Prefix Tuning集成到BERT中\n",
    "prefix_tuning = PrefixTuning(model)\n",
    "optimizer = optim.Adam(prefix_tuning.parameters(), lr=1e-5)\n",
    "\n",
    "# 准备示例数据\n",
    "text = \"LoRA and Prefix Tuning are efficient methods for adapting large models.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# 模型训练流程\n",
    "prefix_tuning.train()\n",
    "for epoch in range(3):  # 训练3个epoch\n",
    "    optimizer.zero_grad()\n",
    "    outputs = prefix_tuning(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    loss = (last_hidden_states ** 2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# 测试流程\n",
    "prefix_tuning.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = prefix_tuning(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    print(\"Output Embeddings:\", outputs.last_hidden_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimized_version",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "优化版 SVD-LoRA + Prefix Tuning 混合模型\n",
    "改进点：\n",
    "1. 修复参数冻结问题，正确处理 bias\n",
    "2. 添加设备管理优化\n",
    "3. 优化训练参数筛选逻辑\n",
    "4. 添加梯度累积支持\n",
    "5. 添加学习率调度器\n",
    "6. 添加模型保存/加载功能\n",
    "7. 优化正则化计算效率\n",
    "8. 添加混合精度训练支持\n",
    "9. 更好的代码结构和注释\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler  # 混合精度训练\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "\n",
    "# ============== 1. 设置环境与设备 ==============\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============== 2. 优化的 SVD-LoRA 层 ==============\n",
    "class SVDLoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    SVD-LoRA 层 (创新点：将权重分解为 U * diag(S) * V)\n",
    "    改进：\n",
    "    - 正确冻结原始层参数（包括 bias）\n",
    "    - 使用更稳定的初始化方法\n",
    "    - 支持可配置的 dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, original_layer, rank=8, alpha=16, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # 保存原始层的配置信息\n",
    "        self.in_features = original_layer.in_features\n",
    "        self.out_features = original_layer.out_features\n",
    "        self.has_bias = original_layer.bias is not None\n",
    "        \n",
    "        # 冻结原始权重但不替换层（保留原始层结构）\n",
    "        self.original_weight = original_layer.weight.data.clone()\n",
    "        original_layer.weight.requires_grad = False\n",
    "        \n",
    "        if self.has_bias:\n",
    "            self.original_bias = original_layer.bias.data.clone()\n",
    "            original_layer.bias.requires_grad = False\n",
    "        else:\n",
    "            self.original_bias = None\n",
    "        \n",
    "        # SVD 分解结构：Delta W = U * diag(S) * V\n",
    "        # 使用 Kaiming 初始化，提高训练稳定性\n",
    "        self.U = nn.Parameter(torch.randn(self.out_features, rank) * 0.02)\n",
    "        self.S = nn.Parameter(torch.ones(rank))  # 奇异值对角线\n",
    "        self.V = nn.Parameter(torch.randn(rank, self.in_features) * 0.02)\n",
    "        \n",
    "        self.rank = rank\n",
    "        self.scaling = alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 原始权重路径: 使用 F.linear 进行高效计算\n",
    "        if self.has_bias:\n",
    "            original_output = F.linear(x, self.original_weight, self.original_bias)\n",
    "        else:\n",
    "            original_output = F.linear(x, self.original_weight, None)\n",
    "        \n",
    "        # SVD 旁路路径: x @ (V.T @ diag(S) @ U.T)\n",
    "        # 优化计算顺序: (x @ V.T) * S @ U.T\n",
    "        lora_input = self.dropout(x)\n",
    "        lora_output = (lora_input @ self.V.t()) * self.S\n",
    "        lora_output = lora_output @ self.U.t()\n",
    "        \n",
    "        return original_output + lora_output * self.scaling\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, rank={self.rank}'\n",
    "\n",
    "\n",
    "# ============== 3. 优化的集成模型 ==============\n",
    "class HybridSVDModel(nn.Module):\n",
    "    \"\"\"\n",
    "    集成模型 (Prefix-Tuning + SVD-LoRA)\n",
    "    改进：\n",
    "    - 支持可配置的 LoRA 注入位置\n",
    "    - 支持可学习的 Prefix 位置编码\n",
    "    - 添加 LayerNorm 保持训练稳定\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name=\"bert-base-uncased\", \n",
    "        prefix_len=10, \n",
    "        lora_rank=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.0,\n",
    "        lora_targets=[\"query\", \"value\"]  # 可配置注入位置\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.prefix_len = prefix_len\n",
    "        self.lora_rank = lora_rank\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # 注入 SVD-LoRA 到指定层\n",
    "        self._inject_lora(lora_rank, lora_alpha, lora_dropout, lora_targets)\n",
    "        \n",
    "        # Prefix Embedding (连续向量)\n",
    "        # 使用 BERT 的 embeddings 归一化层\n",
    "        self.prefix_embedding = nn.Parameter(\n",
    "            torch.randn(prefix_len, hidden_size) * 0.02\n",
    "        )\n",
    "        \n",
    "        # Prefix LayerNorm 提高稳定性\n",
    "        self.prefix_ln = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # 任务头 (以分类任务为例)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size // 2, 2)\n",
    "        )\n",
    "    \n",
    "    def _inject_lora(self, rank, alpha, dropout, targets):\n",
    "        \"\"\"注入 LoRA 到指定层\"\"\"\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            if \"query\" in targets:\n",
    "                layer.attention.self.query = SVDLoRALinear(\n",
    "                    layer.attention.self.query, \n",
    "                    rank=rank, \n",
    "                    alpha=alpha,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            if \"value\" in targets:\n",
    "                layer.attention.self.value = SVDLoRALinear(\n",
    "                    layer.attention.self.value, \n",
    "                    rank=rank, \n",
    "                    alpha=alpha,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            # 可选：添加 key 和 output 投影的 LoRA\n",
    "            # if \"key\" in targets:\n",
    "            #     layer.attention.self.key = SVDLoRALinear(...)\n",
    "            # if \"output\" in targets:\n",
    "            #     layer.attention.output.dense = SVDLoRALinear(...)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # 1. 处理 Prefix\n",
    "        raw_embeds = self.bert.embeddings(input_ids)\n",
    "        \n",
    "        # 扩展 prefix 到 batch size\n",
    "        prefix_embeds = self.prefix_embedding.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        prefix_embeds = self.prefix_ln(prefix_embeds)  # LayerNorm 提高稳定性\n",
    "        \n",
    "        # 拼接 prefix 和原始 embeddings\n",
    "        inputs_embeds = torch.cat([prefix_embeds, raw_embeds], dim=1)\n",
    "        \n",
    "        # 2. 扩展 Attention Mask\n",
    "        prefix_mask = torch.ones(batch_size, self.prefix_len, device=inputs_embeds.device)\n",
    "        full_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # 3. 经过 BERT (含 LoRA 旁路)\n",
    "        outputs = self.bert(inputs_embeds=inputs_embeds, attention_mask=full_mask)\n",
    "        \n",
    "        # 取 [CLS] token 的输出进行分类\n",
    "        # 注意：由于加了 prefix，[CLS] 的位置现在在 index = self.prefix_len\n",
    "        cls_output = outputs.last_hidden_state[:, self.prefix_len, :]\n",
    "        return self.classifier(cls_output)\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"获取可训练参数\"\"\"\n",
    "        trainable_params = []\n",
    "        for n, p in self.named_parameters():\n",
    "            # 精确匹配 LoRA 参数、Prefix 参数和 Classifier 参数\n",
    "            if '.U' in n or '.V' in n or '.S' in n or 'prefix' in n or 'classifier' in n:\n",
    "                if p.requires_grad:\n",
    "                    trainable_params.append(p)\n",
    "        return trainable_params\n",
    "    \n",
    "    def get_svd_params(self):\n",
    "        \"\"\"获取 SVD 奇异值参数（用于正则化）\"\"\"\n",
    "        return [p for n, p in self.named_parameters() if '.S' in n and p.requires_grad]\n",
    "\n",
    "\n",
    "# ============== 4. 训练器类 ==============\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    训练器封装\n",
    "    支持：\n",
    "    - 梯度累积\n",
    "    - 混合精度训练\n",
    "    - 学习率调度\n",
    "    - 模型保存/加载\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        use_amp=True  # 混合精度\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.use_amp = use_amp and device.type == \"cuda\"\n",
    "        \n",
    "        # 优化器配置\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.get_trainable_params(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 混合精度训练\n",
    "        self.scaler = GradScaler() if self.use_amp else None\n",
    "    \n",
    "    def train_step(self, inputs, labels, svd_reg_lambda=0.01):\n",
    "        \"\"\"单步训练\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # 前向传播\n",
    "        if self.use_amp:\n",
    "            with autocast():\n",
    "                logits = self.model(inputs['input_ids'], inputs['attention_mask'])\n",
    "                loss = self.criterion(logits, labels)\n",
    "                \n",
    "                # SVD 奇异值稀疏化正则化\n",
    "                svd_params = self.model.get_svd_params()\n",
    "                if svd_params:\n",
    "                    l1_reg = sum(torch.norm(p, 1) for p in svd_params)\n",
    "                    loss = loss + svd_reg_lambda * l1_reg\n",
    "        else:\n",
    "            logits = self.model(inputs['input_ids'], inputs['attention_mask'])\n",
    "            loss = self.criterion(logits, labels)\n",
    "            \n",
    "            # SVD 奇异值稀疏化正则化\n",
    "            svd_params = self.model.get_svd_params()\n",
    "            if svd_params:\n",
    "                l1_reg = sum(torch.norm(p, 1) for p in svd_params)\n",
    "                loss = loss + svd_reg_lambda * l1_reg\n",
    "        \n",
    "        # 梯度缩放（混合精度）\n",
    "        loss = loss / self.gradient_accumulation_steps\n",
    "        \n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        return loss.item() * self.gradient_accumulation_steps\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"优化器步骤\"\"\"\n",
    "        if self.use_amp:\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "    \n",
    "    def set_scheduler(self, num_training_steps, num_warmup_steps):\n",
    "        \"\"\"设置学习率调度器\"\"\"\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "# ============== 5. 主训练流程 ==============\n",
    "def main():\n",
    "    # 模型配置\n",
    "    model_config = {\n",
    "        \"model_name\": \"bert-base-uncased\",\n",
    "        \"prefix_len\": 10,\n",
    "        \"lora_rank\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"lora_targets\": [\"query\", \"value\"]\n",
    "    }\n",
    "    \n",
    "    # 训练配置\n",
    "    train_config = {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"num_epochs\": 10,\n",
    "        \"svd_reg_lambda\": 0.01\n",
    "    }\n",
    "    \n",
    "    # 初始化 tokenizer 和模型\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_config[\"model_name\"])\n",
    "    model = HybridSVDModel(**model_config).to(device)\n",
    "    \n",
    "    # 打印可训练参数数量\n",
    "    trainable_params = model.get_trainable_params()\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params_count = sum(p.numel() for p in trainable_params)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params_count:,} ({100*trainable_params_count/total_params:.2f}%)\")\n",
    "    \n",
    "    # 初始化训练器\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        learning_rate=train_config[\"learning_rate\"],\n",
    "        weight_decay=train_config[\"weight_decay\"],\n",
    "        warmup_ratio=train_config[\"warmup_ratio\"],\n",
    "        gradient_accumulation_steps=train_config[\"gradient_accumulation_steps\"]\n",
    "    )\n",
    "    \n",
    "    # 准备示例数据\n",
    "    texts = [\"The movie was fantastic!\", \"I hated this film.\", \n",
    "             \"Great acting and plot!\", \"Boring and predictable.\"]\n",
    "    labels = torch.tensor([1, 0, 1, 0]).to(device)\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # 设置学习率调度器\n",
    "    num_training_steps = (len(texts) // train_config[\"gradient_accumulation_steps\"]) * train_config[\"num_epochs\"]\n",
    "    num_warmup_steps = int(num_training_steps * train_config[\"warmup_ratio\"])\n",
    "    trainer.set_scheduler(num_training_steps, num_warmup_steps)\n",
    "    \n",
    "    # 训练循环\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    global_step = 0\n",
    "    for epoch in range(train_config[\"num_epochs\"]):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # 模拟多个 batch\n",
    "        for step in range(0, len(texts), 2):\n",
    "            batch_inputs = {\n",
    "                'input_ids': inputs['input_ids'][step:step+2],\n",
    "                'attention_mask': inputs['attention_mask'][step:step+2]\n",
    "            }\n",
    "            batch_labels = labels[step:step+2]\n",
    "            \n",
    "            loss = trainer.train_step(batch_inputs, batch_labels, train_config[\"svd_reg_lambda\"])\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # 梯度累积步骤\n",
    "            if (step + 2) % (2 * train_config[\"gradient_accumulation_steps\"]) == 0:\n",
    "                trainer.step()\n",
    "                trainer.scheduler.step()\n",
    "                global_step += 1\n",
    "        \n",
    "        # 打印 epoch 信息\n",
    "        avg_loss = epoch_loss / (len(texts) // 2)\n",
    "        current_lr = trainer.scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch {epoch+1}/{train_config['num_epochs']} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # 观察奇异值 (模型解释性)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Top 5 Singular Values in Layer 0 Query:\")\n",
    "    print(model.bert.encoder.layer[0].attention.self.query.S[:5].detach().cpu().numpy())\n",
    "    \n",
    "    # 保存模型\n",
    "    trainer.save_model(\"hybrid_svd_model.pt\")\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 需要导入 F\n",
    "    import torch.nn.functional as F\n",
    "    model, trainer = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
